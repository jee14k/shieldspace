{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================\n",
      "üìÇ Dataset: aggression_parsed_dataset.csv\n",
      "============================\n",
      "\n",
      "üìê Structure:\n",
      "index           int64\n",
      "Text           object\n",
      "ed_label_0    float64\n",
      "ed_label_1    float64\n",
      "oh_label        int64\n",
      "dtype: object\n",
      "\n",
      "üìä Summary:\n",
      "                index     Text     ed_label_0     ed_label_1       oh_label\n",
      "count   115864.000000   115864  115864.000000  115864.000000  115864.000000\n",
      "unique            NaN   115661            NaN            NaN            NaN\n",
      "top               NaN  Err:509            NaN            NaN            NaN\n",
      "freq              NaN       26            NaN            NaN            NaN\n",
      "mean     57931.500000      NaN       0.814172       0.185828       0.127581\n",
      "std      33447.200132      NaN       0.271089       0.271089       0.333624\n",
      "min          0.000000      NaN       0.000000       0.000000       0.000000\n",
      "25%      28965.750000      NaN       0.750000       0.000000       0.000000\n",
      "50%      57931.500000      NaN       0.900000       0.100000       0.000000\n",
      "75%      86897.250000      NaN       1.000000       0.250000       0.000000\n",
      "max     115863.000000      NaN       1.000000       1.000000       1.000000\n",
      "\n",
      "üîé First 5 Rows:\n",
      "   index                                               Text  ed_label_0  \\\n",
      "0      0  `- This is not ``creative``.  Those are the di...    0.900000   \n",
      "1      1  `  :: the term ``standard model`` is itself le...    1.000000   \n",
      "2      2    True or false, the situation as of March 200...    1.000000   \n",
      "3      3   Next, maybe you could work on being less cond...    0.555556   \n",
      "4      4               This page will need disambiguation.     1.000000   \n",
      "\n",
      "   ed_label_1  oh_label  \n",
      "0    0.100000         0  \n",
      "1    0.000000         0  \n",
      "2    0.000000         0  \n",
      "3    0.444444         0  \n",
      "4    0.000000         0  \n",
      "\n",
      "üìå Label Distribution (oh_label):\n",
      "oh_label\n",
      "0    101082\n",
      "1     14782\n",
      "Name: count, dtype: int64\n",
      "\n",
      "============================\n",
      "üìÇ Dataset: attack_parsed_dataset.csv\n",
      "============================\n",
      "\n",
      "üìê Structure:\n",
      "index           int64\n",
      "Text           object\n",
      "ed_label_0    float64\n",
      "ed_label_1    float64\n",
      "oh_label        int64\n",
      "dtype: object\n",
      "\n",
      "üìä Summary:\n",
      "                index     Text     ed_label_0     ed_label_1       oh_label\n",
      "count   115864.000000   115864  115864.000000  115864.000000  115864.000000\n",
      "unique            NaN   115661            NaN            NaN            NaN\n",
      "top               NaN  Err:509            NaN            NaN            NaN\n",
      "freq              NaN       26            NaN            NaN            NaN\n",
      "mean     57931.500000      NaN       0.830222       0.169778       0.117293\n",
      "std      33447.200132      NaN       0.264987       0.264987       0.321770\n",
      "min          0.000000      NaN       0.000000       0.000000       0.000000\n",
      "25%      28965.750000      NaN       0.777778       0.000000       0.000000\n",
      "50%      57931.500000      NaN       0.950000       0.050000       0.000000\n",
      "75%      86897.250000      NaN       1.000000       0.222222       0.000000\n",
      "max     115863.000000      NaN       1.000000       1.000000       1.000000\n",
      "\n",
      "üîé First 5 Rows:\n",
      "   index                                               Text  ed_label_0  \\\n",
      "0      0  `- This is not ``creative``.  Those are the di...    1.000000   \n",
      "1      1  `  :: the term ``standard model`` is itself le...    1.000000   \n",
      "2      2    True or false, the situation as of March 200...    1.000000   \n",
      "3      3   Next, maybe you could work on being less cond...    0.555556   \n",
      "4      4               This page will need disambiguation.     1.000000   \n",
      "\n",
      "   ed_label_1  oh_label  \n",
      "0    0.000000         0  \n",
      "1    0.000000         0  \n",
      "2    0.000000         0  \n",
      "3    0.444444         0  \n",
      "4    0.000000         0  \n",
      "\n",
      "üìå Label Distribution (oh_label):\n",
      "oh_label\n",
      "0    102274\n",
      "1     13590\n",
      "Name: count, dtype: int64\n",
      "\n",
      "============================\n",
      "üìÇ Dataset: kaggle_parsed_dataset.csv\n",
      "============================\n",
      "\n",
      "üìê Structure:\n",
      "index        int64\n",
      "oh_label     int64\n",
      "Date        object\n",
      "Text        object\n",
      "dtype: object\n",
      "\n",
      "üìä Summary:\n",
      "              index     oh_label             Date  \\\n",
      "count   8799.000000  8799.000000             7557   \n",
      "unique          NaN          NaN             7440   \n",
      "top             NaN          NaN  20120529160237Z   \n",
      "freq            NaN          NaN                3   \n",
      "mean    4399.000000     0.318900              NaN   \n",
      "std     2540.196843     0.466077              NaN   \n",
      "min        0.000000     0.000000              NaN   \n",
      "25%     2199.500000     0.000000              NaN   \n",
      "50%     4399.000000     0.000000              NaN   \n",
      "75%     6598.500000     1.000000              NaN   \n",
      "max     8798.000000     1.000000              NaN   \n",
      "\n",
      "                                                     Text  \n",
      "count                                                8799  \n",
      "unique                                               8745  \n",
      "top     \"dan_amd\\n\\nYou have realy no clue on every si...  \n",
      "freq                                                    7  \n",
      "mean                                                  NaN  \n",
      "std                                                   NaN  \n",
      "min                                                   NaN  \n",
      "25%                                                   NaN  \n",
      "50%                                                   NaN  \n",
      "75%                                                   NaN  \n",
      "max                                                   NaN  \n",
      "\n",
      "üîé First 5 Rows:\n",
      "   index  oh_label             Date  \\\n",
      "0      0         1  20120618192155Z   \n",
      "1      1         0  20120528192215Z   \n",
      "2      2         0              NaN   \n",
      "3      3         0              NaN   \n",
      "4      4         0  20120619094753Z   \n",
      "\n",
      "                                                Text  \n",
      "0                               \"You fuck your dad.\"  \n",
      "1  \"i really don't understand your point.\\xa0 It ...  \n",
      "2  \"A\\\\xc2\\\\xa0majority of Canadians can and has ...  \n",
      "3  \"listen if you dont wanna get married to a man...  \n",
      "4  \"C\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1edd...  \n",
      "\n",
      "üìå Label Distribution (oh_label):\n",
      "oh_label\n",
      "0    5993\n",
      "1    2806\n",
      "Name: count, dtype: int64\n",
      "\n",
      "============================\n",
      "üìÇ Dataset: toxicity_parsed_dataset.csv\n",
      "============================\n",
      "\n",
      "üìê Structure:\n",
      "index           int64\n",
      "Text           object\n",
      "ed_label_0    float64\n",
      "ed_label_1    float64\n",
      "oh_label        int64\n",
      "dtype: object\n",
      "\n",
      "üìä Summary:\n",
      "               index     Text     ed_label_0     ed_label_1       oh_label\n",
      "count   159686.00000   159686  159686.000000  159686.000000  159686.000000\n",
      "unique           NaN   159388            NaN            NaN            NaN\n",
      "top              NaN  Err:509            NaN            NaN            NaN\n",
      "freq             NaN       34            NaN            NaN            NaN\n",
      "mean     79842.50000      NaN       0.854951       0.145049       0.096201\n",
      "std      46097.52188      NaN       0.253866       0.253866       0.294868\n",
      "min          0.00000      NaN       0.000000       0.000000       0.000000\n",
      "25%      39921.25000      NaN       0.800000       0.000000       0.000000\n",
      "50%      79842.50000      NaN       1.000000       0.000000       0.000000\n",
      "75%     119763.75000      NaN       1.000000       0.200000       0.000000\n",
      "max     159685.00000      NaN       1.000000       1.000000       1.000000\n",
      "\n",
      "üîé First 5 Rows:\n",
      "   index                                               Text  ed_label_0  \\\n",
      "0      0  This: :One can make an analogy in mathematical...         0.9   \n",
      "1      1  `  :Clarification for you  (and Zundark's righ...         1.0   \n",
      "2      2                          Elected or Electoral? JHK         1.0   \n",
      "3      3  `This is such a fun entry.   Devotchka  I once...         1.0   \n",
      "4      4  Please relate the ozone hole to increases in c...         0.8   \n",
      "\n",
      "   ed_label_1  oh_label  \n",
      "0         0.1         0  \n",
      "1         0.0         0  \n",
      "2         0.0         0  \n",
      "3         0.0         0  \n",
      "4         0.2         0  \n",
      "\n",
      "üìå Label Distribution (oh_label):\n",
      "oh_label\n",
      "0    144324\n",
      "1     15362\n",
      "Name: count, dtype: int64\n",
      "\n",
      "============================\n",
      "üìÇ Dataset: twitter_parsed_dataset.csv\n",
      "============================\n",
      "\n",
      "üìê Structure:\n",
      "index          object\n",
      "id             object\n",
      "Text           object\n",
      "Annotation     object\n",
      "oh_label      float64\n",
      "dtype: object\n",
      "\n",
      "üìä Summary:\n",
      "                        index                     id  \\\n",
      "count                   16851                  16850   \n",
      "unique                  16851                  16850   \n",
      "top     5.74948705591165E+017  5.74948705591165E+017   \n",
      "freq                        1                      1   \n",
      "mean                      NaN                    NaN   \n",
      "std                       NaN                    NaN   \n",
      "min                       NaN                    NaN   \n",
      "25%                       NaN                    NaN   \n",
      "50%                       NaN                    NaN   \n",
      "75%                       NaN                    NaN   \n",
      "max                       NaN                    NaN   \n",
      "\n",
      "                                                     Text Annotation  \\\n",
      "count                                               16850      16848   \n",
      "unique                                              16850          3   \n",
      "top     @halalflaws @biebervalue @greenlinerzjm I read...       none   \n",
      "freq                                                    1      11501   \n",
      "mean                                                  NaN        NaN   \n",
      "std                                                   NaN        NaN   \n",
      "min                                                   NaN        NaN   \n",
      "25%                                                   NaN        NaN   \n",
      "50%                                                   NaN        NaN   \n",
      "75%                                                   NaN        NaN   \n",
      "max                                                   NaN        NaN   \n",
      "\n",
      "            oh_label  \n",
      "count   16848.000000  \n",
      "unique           NaN  \n",
      "top              NaN  \n",
      "freq             NaN  \n",
      "mean        0.317367  \n",
      "std         0.465465  \n",
      "min         0.000000  \n",
      "25%         0.000000  \n",
      "50%         0.000000  \n",
      "75%         1.000000  \n",
      "max         1.000000  \n",
      "\n",
      "üîé First 5 Rows:\n",
      "                   index                     id  \\\n",
      "0  5.74948705591165E+017  5.74948705591165E+017   \n",
      "1  5.71917888690393E+017  5.71917888690393E+017   \n",
      "2  3.90255841338601E+017  3.90255841338601E+017   \n",
      "3  5.68208850655916E+017  5.68208850655916E+017   \n",
      "4  5.75596338802373E+017  5.75596338802373E+017   \n",
      "\n",
      "                                                Text Annotation  oh_label  \n",
      "0  @halalflaws @biebervalue @greenlinerzjm I read...       none       0.0  \n",
      "1  @ShreyaBafna3 Now you idiots claim that people...       none       0.0  \n",
      "2  RT @Mooseoftorment Call me sexist, but when I ...     sexism       1.0  \n",
      "3  @g0ssipsquirrelx Wrong, ISIS follows the examp...     racism       1.0  \n",
      "4                             #mkr No No No No No No       none       0.0  \n",
      "\n",
      "üìå Label Distribution (oh_label):\n",
      "oh_label\n",
      "0.0    11501\n",
      "1.0     5347\n",
      "NaN        3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "============================\n",
      "üìÇ Dataset: twitter_racism_parsed_dataset.csv\n",
      "============================\n",
      "\n",
      "üìê Structure:\n",
      "index         float64\n",
      "id            float64\n",
      "Text           object\n",
      "Annotation     object\n",
      "oh_label        int64\n",
      "dtype: object\n",
      "\n",
      "üìä Summary:\n",
      "               index            id  \\\n",
      "count   1.347100e+04  1.347100e+04   \n",
      "unique           NaN           NaN   \n",
      "top              NaN           NaN   \n",
      "freq             NaN           NaN   \n",
      "mean    5.645450e+17  5.645450e+17   \n",
      "std     2.263979e+16  2.263979e+16   \n",
      "min     3.208178e+17  3.208178e+17   \n",
      "25%     5.637581e+17  5.637581e+17   \n",
      "50%     5.714068e+17  5.714068e+17   \n",
      "75%     5.755013e+17  5.755013e+17   \n",
      "max     5.922733e+17  5.922733e+17   \n",
      "\n",
      "                                                     Text Annotation  \\\n",
      "count                                               13471      13471   \n",
      "unique                                              13471          2   \n",
      "top     @AAlwuhaib1977 Muslim mob violence against Hin...       none   \n",
      "freq                                                    1      11501   \n",
      "mean                                                  NaN        NaN   \n",
      "std                                                   NaN        NaN   \n",
      "min                                                   NaN        NaN   \n",
      "25%                                                   NaN        NaN   \n",
      "50%                                                   NaN        NaN   \n",
      "75%                                                   NaN        NaN   \n",
      "max                                                   NaN        NaN   \n",
      "\n",
      "           oh_label  \n",
      "count   13471.00000  \n",
      "unique          NaN  \n",
      "top             NaN  \n",
      "freq            NaN  \n",
      "mean        0.14624  \n",
      "std         0.35336  \n",
      "min         0.00000  \n",
      "25%         0.00000  \n",
      "50%         0.00000  \n",
      "75%         0.00000  \n",
      "max         1.00000  \n",
      "\n",
      "üîé First 5 Rows:\n",
      "          index            id  \\\n",
      "0  5.767493e+17  5.767493e+17   \n",
      "1  5.408905e+17  5.408905e+17   \n",
      "2  5.678433e+17  5.678433e+17   \n",
      "3  5.766462e+17  5.766462e+17   \n",
      "4  5.713492e+17  5.713492e+17   \n",
      "\n",
      "                                                Text Annotation  oh_label  \n",
      "0  @AAlwuhaib1977 Muslim mob violence against Hin...     racism         1  \n",
      "1             @Te4m_NiGhtM4Re http://t.co/5Ih7MkDbQG       none         0  \n",
      "2  @jncatron @isra_jourisra @AMPalestine Islamoph...     racism         1  \n",
      "3  Finally I'm all caught up, and that sudden dea...       none         0  \n",
      "4             @carolinesinders @herecomesfran *hugs*       none         0  \n",
      "\n",
      "üìå Label Distribution (oh_label):\n",
      "oh_label\n",
      "0    11501\n",
      "1     1970\n",
      "Name: count, dtype: int64\n",
      "\n",
      "============================\n",
      "üìÇ Dataset: twitter_sexism_parsed_dataset.csv\n",
      "============================\n",
      "\n",
      "üìê Structure:\n",
      "index          object\n",
      "id             object\n",
      "Text           object\n",
      "Annotation     object\n",
      "oh_label      float64\n",
      "dtype: object\n",
      "\n",
      "üìä Summary:\n",
      "                        index                     id  \\\n",
      "count                   14881                  14880   \n",
      "unique                  14881                  14880   \n",
      "top     5.35198627292254E+017  5.35198627292254E+017   \n",
      "freq                        1                      1   \n",
      "mean                      NaN                    NaN   \n",
      "std                       NaN                    NaN   \n",
      "min                       NaN                    NaN   \n",
      "25%                       NaN                    NaN   \n",
      "50%                       NaN                    NaN   \n",
      "75%                       NaN                    NaN   \n",
      "max                       NaN                    NaN   \n",
      "\n",
      "                                                     Text Annotation  \\\n",
      "count                                               14880      14878   \n",
      "unique                                              14880          2   \n",
      "top     RT @BeepsS: @senna1 @BeepsS: I'm not sexist bu...       none   \n",
      "freq                                                    1      11501   \n",
      "mean                                                  NaN        NaN   \n",
      "std                                                   NaN        NaN   \n",
      "min                                                   NaN        NaN   \n",
      "25%                                                   NaN        NaN   \n",
      "50%                                                   NaN        NaN   \n",
      "75%                                                   NaN        NaN   \n",
      "max                                                   NaN        NaN   \n",
      "\n",
      "            oh_label  \n",
      "count   14878.000000  \n",
      "unique           NaN  \n",
      "top              NaN  \n",
      "freq             NaN  \n",
      "mean        0.226979  \n",
      "std         0.418893  \n",
      "min         0.000000  \n",
      "25%         0.000000  \n",
      "50%         0.000000  \n",
      "75%         0.000000  \n",
      "max         1.000000  \n",
      "\n",
      "üîé First 5 Rows:\n",
      "                   index                     id  \\\n",
      "0  5.35198627292254E+017  5.35198627292254E+017   \n",
      "1  5.75984924030714E+017  5.75984924030714E+017   \n",
      "2   5.7233536016588E+017   5.7233536016588E+017   \n",
      "3  5.72337925708374E+017  5.72337925708374E+017   \n",
      "4  4.43033024528011E+017  4.43033024528011E+017   \n",
      "\n",
      "                                                Text Annotation  oh_label  \n",
      "0  RT @BeepsS: @senna1 @BeepsS: I'm not sexist bu...     sexism       1.0  \n",
      "1   There's some very hate able teams this year #MKR       none       0.0  \n",
      "2  RT @The_Eccles: \"Everyone underestimated us\" \\...       none       0.0  \n",
      "3  RT @NOTLukeDarcy: did @Channel7 or #MKR actual...       none       0.0  \n",
      "4  No, you don't. @Shut_Up_Jeff: I thought of a r...     sexism       1.0  \n",
      "\n",
      "üìå Label Distribution (oh_label):\n",
      "oh_label\n",
      "0.0    11501\n",
      "1.0     3377\n",
      "NaN        3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "============================\n",
      "üìÇ Dataset: youtube_parsed_dataset.csv\n",
      "============================\n",
      "\n",
      "üìê Structure:\n",
      "index                     int64\n",
      "UserIndex                object\n",
      "Text                     object\n",
      "Number of Comments        int64\n",
      "Number of Subscribers     int64\n",
      "Membership Duration       int64\n",
      "Number of Uploads         int64\n",
      "Profanity in UserID       int64\n",
      "Age                       int64\n",
      "oh_label                  int64\n",
      "dtype: object\n",
      "\n",
      "üìä Summary:\n",
      "              index UserIndex  Text  Number of Comments  \\\n",
      "count   3464.000000      3464  3464         3464.000000   \n",
      "unique          NaN      3464  3462                 NaN   \n",
      "top             NaN        X1   ...                 NaN   \n",
      "freq            NaN         1     3                 NaN   \n",
      "mean    1733.021651       NaN   NaN           15.452367   \n",
      "std     1001.844201       NaN   NaN           10.862517   \n",
      "min        0.000000       NaN   NaN            1.000000   \n",
      "25%      865.750000       NaN   NaN            6.000000   \n",
      "50%     1731.500000       NaN   NaN           14.000000   \n",
      "75%     2601.250000       NaN   NaN           23.000000   \n",
      "max     3468.000000       NaN   NaN           50.000000   \n",
      "\n",
      "        Number of Subscribers  Membership Duration  Number of Uploads  \\\n",
      "count             3464.000000          3464.000000        3464.000000   \n",
      "unique                    NaN                  NaN                NaN   \n",
      "top                       NaN                  NaN                NaN   \n",
      "freq                      NaN                  NaN                NaN   \n",
      "mean               304.318995             3.714781          10.288395   \n",
      "std              15520.532319             1.392837          28.646525   \n",
      "min                  0.000000             2.000000           1.000000   \n",
      "25%                  0.000000             3.000000           5.000000   \n",
      "50%                  2.000000             3.000000           5.000000   \n",
      "75%                  7.000000             4.000000           5.000000   \n",
      "max             912377.000000             9.000000         820.000000   \n",
      "\n",
      "        Profanity in UserID          Age     oh_label  \n",
      "count           3464.000000  3464.000000  3464.000000  \n",
      "unique                  NaN          NaN          NaN  \n",
      "top                     NaN          NaN          NaN  \n",
      "freq                    NaN          NaN          NaN  \n",
      "mean               0.113164    24.879042     0.120381  \n",
      "std                0.316839    13.286361     0.325454  \n",
      "min                0.000000    13.000000     0.000000  \n",
      "25%                0.000000    18.000000     0.000000  \n",
      "50%                0.000000    21.000000     0.000000  \n",
      "75%                0.000000    27.000000     0.000000  \n",
      "max                1.000000   112.000000     1.000000  \n",
      "\n",
      "üîé First 5 Rows:\n",
      "   index UserIndex                                               Text  \\\n",
      "0      0        X1  Does N.e.bodyelse Hear her Crazy ass Screamin ...   \n",
      "1      1        X2  There are so many things that are incorrect wi...   \n",
      "2      2        X3  3:26 hahah my boyfriend showed this song to me...   \n",
      "3      3     X2218  dick beyonce fuck y a ass hole you are truely ...   \n",
      "4      4        X5  DongHaeTaemin and Kai ;A; luhansehun and bacon...   \n",
      "\n",
      "   Number of Comments  Number of Subscribers  Membership Duration  \\\n",
      "0                  10                      1                    3   \n",
      "1                   3                      0                    6   \n",
      "2                   7                      0                    3   \n",
      "3                  34                      0                    3   \n",
      "4                  11                    173                    5   \n",
      "\n",
      "   Number of Uploads  Profanity in UserID  Age  oh_label  \n",
      "0                  3                    0   15         0  \n",
      "1                  5                    0   31         0  \n",
      "2                  5                    0   43         1  \n",
      "3                  5                    0   44         1  \n",
      "4                  5                    0   21         0  \n",
      "\n",
      "üìå Label Distribution (oh_label):\n",
      "oh_label\n",
      "0    3047\n",
      "1     417\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# List of CSV file names present in the folder.\n",
    "csv_file_list = [\n",
    "    \"aggression_parsed_dataset.csv\",\n",
    "    \"attack_parsed_dataset.csv\",\n",
    "    \"Cyberbullying_Dataset_Summary_Table__Detailed_.csv\",\n",
    "    \"kaggle_parsed_dataset.csv\",\n",
    "    \"twitter_sexism_parsed_dataset.csv\",\n",
    "    \"twitter_racism_parsed_dataset.csv\",\n",
    "    \"twitter_parsed_dataset.csv\",\n",
    "    \"toxicity_parsed_dataset.csv\",\n",
    "    \"youtube_parsed_dataset.csv\"\n",
    "]\n",
    "\n",
    "# Loop through each CSV file in the list.\n",
    "for file_name in csv_file_list:\n",
    "    # Since the CSV files are in the same folder as the notebook,\n",
    "    # the file path is simply the file name.\n",
    "    file_path = os.path.join(file_name)\n",
    "    \n",
    "    # Print header for current file processing.\n",
    "    print(f\"=== Processing File: {file_name} ===\")\n",
    "    \n",
    "    # Try to read the CSV file into a pandas DataFrame.\n",
    "    try:\n",
    "        data_frame = pd.read_csv(file_path)\n",
    "    except Exception as error:\n",
    "        print(f\"Error reading {file_name}: {error}\")\n",
    "        print(\"-\" * 60 + \"\\n\")\n",
    "        continue  # Skip to the next file if an error occurs.\n",
    "    \n",
    "    # Display the column names of the DataFrame.\n",
    "    print(\"Column Names:\")\n",
    "    print(data_frame.columns.tolist())\n",
    "    \n",
    "    # Display the data types of each column.\n",
    "    print(\"\\nData Types:\")\n",
    "    print(data_frame.dtypes)\n",
    "    \n",
    "    # Display the number of rows and columns in the DataFrame.\n",
    "    num_rows, num_columns = data_frame.shape\n",
    "    print(f\"\\nNumber of Rows: {num_rows}\")\n",
    "    print(f\"Number of Columns: {num_columns}\")\n",
    "    \n",
    "    # Display the first 5 rows of the DataFrame as a preview.\n",
    "    print(\"\\nFirst 5 Rows:\")\n",
    "    print(data_frame.head())\n",
    "    \n",
    "    # Print a separator line after processing each file.\n",
    "    print(\"\\n\" + \"-\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color: #FF5733; font-size: 36px;\">Digital Defender: Game Wrangling and Analysis File</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color: #FF5733; font-size: 19px;\"> Data details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script uses the os and pandas libraries to manage and process multiple CSV files stored in the same directory. It iterates over a list of CSV filenames, reads each file into a pandas DataFrame, and then prints essential details such as column names, data types, the shape of the DataFrame, and a preview of the first few rows. If a file can‚Äôt be read due to an error, the code prints an error message and continues with the next file, ensuring a clear separation between the outputs of each file. This setup provides a quick and efficient way to explore and understand the structure and content of your CSV data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Processing File: aggression_parsed_dataset.csv ===\n",
      "Column Names:\n",
      "['index', 'Text', 'ed_label_0', 'ed_label_1', 'oh_label']\n",
      "\n",
      "Data Types:\n",
      "index           int64\n",
      "Text           object\n",
      "ed_label_0    float64\n",
      "ed_label_1    float64\n",
      "oh_label        int64\n",
      "dtype: object\n",
      "\n",
      "Number of Rows: 115864\n",
      "Number of Columns: 5\n",
      "\n",
      "First 5 Rows:\n",
      "   index                                               Text  ed_label_0  \\\n",
      "0      0  `- This is not ``creative``.  Those are the di...    0.900000   \n",
      "1      1  `  :: the term ``standard model`` is itself le...    1.000000   \n",
      "2      2    True or false, the situation as of March 200...    1.000000   \n",
      "3      3   Next, maybe you could work on being less cond...    0.555556   \n",
      "4      4               This page will need disambiguation.     1.000000   \n",
      "\n",
      "   ed_label_1  oh_label  \n",
      "0    0.100000         0  \n",
      "1    0.000000         0  \n",
      "2    0.000000         0  \n",
      "3    0.444444         0  \n",
      "4    0.000000         0  \n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Processing File: attack_parsed_dataset.csv ===\n",
      "Column Names:\n",
      "['index', 'Text', 'ed_label_0', 'ed_label_1', 'oh_label']\n",
      "\n",
      "Data Types:\n",
      "index           int64\n",
      "Text           object\n",
      "ed_label_0    float64\n",
      "ed_label_1    float64\n",
      "oh_label        int64\n",
      "dtype: object\n",
      "\n",
      "Number of Rows: 115864\n",
      "Number of Columns: 5\n",
      "\n",
      "First 5 Rows:\n",
      "   index                                               Text  ed_label_0  \\\n",
      "0      0  `- This is not ``creative``.  Those are the di...    1.000000   \n",
      "1      1  `  :: the term ``standard model`` is itself le...    1.000000   \n",
      "2      2    True or false, the situation as of March 200...    1.000000   \n",
      "3      3   Next, maybe you could work on being less cond...    0.555556   \n",
      "4      4               This page will need disambiguation.     1.000000   \n",
      "\n",
      "   ed_label_1  oh_label  \n",
      "0    0.000000         0  \n",
      "1    0.000000         0  \n",
      "2    0.000000         0  \n",
      "3    0.444444         0  \n",
      "4    0.000000         0  \n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Processing File: Cyberbullying_Dataset_Summary_Table__Detailed_.csv ===\n",
      "Column Names:\n",
      "['Names', 'File Type', 'Link address', 'Description']\n",
      "\n",
      "Data Types:\n",
      "Names           object\n",
      "File Type       object\n",
      "Link address    object\n",
      "Description     object\n",
      "dtype: object\n",
      "\n",
      "Number of Rows: 8\n",
      "Number of Columns: 4\n",
      "\n",
      "First 5 Rows:\n",
      "                Names File Type           Link address  \\\n",
      "0  Aggression dataset       CSV  User uploaded locally   \n",
      "1      Attack dataset       CSV  User uploaded locally   \n",
      "2    Toxicity dataset       CSV  User uploaded locally   \n",
      "3      Kaggle dataset       CSV  User uploaded locally   \n",
      "4     Twitter dataset       CSV  User uploaded locally   \n",
      "\n",
      "                                         Description  \n",
      "0  Contains user-generated comments along with ma...  \n",
      "1  Similar to the aggression dataset, this file c...  \n",
      "2  This dataset is geared toward detecting toxic ...  \n",
      "3  A streamlined dataset from Kaggle containing t...  \n",
      "4  This dataset includes text messages with annot...  \n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Processing File: kaggle_parsed_dataset.csv ===\n",
      "Column Names:\n",
      "['index', 'oh_label', 'Date', 'Text']\n",
      "\n",
      "Data Types:\n",
      "index        int64\n",
      "oh_label     int64\n",
      "Date        object\n",
      "Text        object\n",
      "dtype: object\n",
      "\n",
      "Number of Rows: 8799\n",
      "Number of Columns: 4\n",
      "\n",
      "First 5 Rows:\n",
      "   index  oh_label             Date  \\\n",
      "0      0         1  20120618192155Z   \n",
      "1      1         0  20120528192215Z   \n",
      "2      2         0              NaN   \n",
      "3      3         0              NaN   \n",
      "4      4         0  20120619094753Z   \n",
      "\n",
      "                                                Text  \n",
      "0                               \"You fuck your dad.\"  \n",
      "1  \"i really don't understand your point.\\xa0 It ...  \n",
      "2  \"A\\\\xc2\\\\xa0majority of Canadians can and has ...  \n",
      "3  \"listen if you dont wanna get married to a man...  \n",
      "4  \"C\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1edd...  \n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Processing File: twitter_sexism_parsed_dataset.csv ===\n",
      "Column Names:\n",
      "['index', 'id', 'Text', 'Annotation', 'oh_label']\n",
      "\n",
      "Data Types:\n",
      "index          object\n",
      "id             object\n",
      "Text           object\n",
      "Annotation     object\n",
      "oh_label      float64\n",
      "dtype: object\n",
      "\n",
      "Number of Rows: 14881\n",
      "Number of Columns: 5\n",
      "\n",
      "First 5 Rows:\n",
      "                   index                     id  \\\n",
      "0  5.35198627292254E+017  5.35198627292254E+017   \n",
      "1  5.75984924030714E+017  5.75984924030714E+017   \n",
      "2   5.7233536016588E+017   5.7233536016588E+017   \n",
      "3  5.72337925708374E+017  5.72337925708374E+017   \n",
      "4  4.43033024528011E+017  4.43033024528011E+017   \n",
      "\n",
      "                                                Text Annotation  oh_label  \n",
      "0  RT @BeepsS: @senna1 @BeepsS: I'm not sexist bu...     sexism       1.0  \n",
      "1   There's some very hate able teams this year #MKR       none       0.0  \n",
      "2  RT @The_Eccles: \"Everyone underestimated us\" \\...       none       0.0  \n",
      "3  RT @NOTLukeDarcy: did @Channel7 or #MKR actual...       none       0.0  \n",
      "4  No, you don't. @Shut_Up_Jeff: I thought of a r...     sexism       1.0  \n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Processing File: twitter_racism_parsed_dataset.csv ===\n",
      "Column Names:\n",
      "['index', 'id', 'Text', 'Annotation', 'oh_label']\n",
      "\n",
      "Data Types:\n",
      "index         float64\n",
      "id            float64\n",
      "Text           object\n",
      "Annotation     object\n",
      "oh_label        int64\n",
      "dtype: object\n",
      "\n",
      "Number of Rows: 13471\n",
      "Number of Columns: 5\n",
      "\n",
      "First 5 Rows:\n",
      "          index            id  \\\n",
      "0  5.767493e+17  5.767493e+17   \n",
      "1  5.408905e+17  5.408905e+17   \n",
      "2  5.678433e+17  5.678433e+17   \n",
      "3  5.766462e+17  5.766462e+17   \n",
      "4  5.713492e+17  5.713492e+17   \n",
      "\n",
      "                                                Text Annotation  oh_label  \n",
      "0  @AAlwuhaib1977 Muslim mob violence against Hin...     racism         1  \n",
      "1             @Te4m_NiGhtM4Re http://t.co/5Ih7MkDbQG       none         0  \n",
      "2  @jncatron @isra_jourisra @AMPalestine Islamoph...     racism         1  \n",
      "3  Finally I'm all caught up, and that sudden dea...       none         0  \n",
      "4             @carolinesinders @herecomesfran *hugs*       none         0  \n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Processing File: twitter_parsed_dataset.csv ===\n",
      "Column Names:\n",
      "['index', 'id', 'Text', 'Annotation', 'oh_label']\n",
      "\n",
      "Data Types:\n",
      "index          object\n",
      "id             object\n",
      "Text           object\n",
      "Annotation     object\n",
      "oh_label      float64\n",
      "dtype: object\n",
      "\n",
      "Number of Rows: 16851\n",
      "Number of Columns: 5\n",
      "\n",
      "First 5 Rows:\n",
      "                   index                     id  \\\n",
      "0  5.74948705591165E+017  5.74948705591165E+017   \n",
      "1  5.71917888690393E+017  5.71917888690393E+017   \n",
      "2  3.90255841338601E+017  3.90255841338601E+017   \n",
      "3  5.68208850655916E+017  5.68208850655916E+017   \n",
      "4  5.75596338802373E+017  5.75596338802373E+017   \n",
      "\n",
      "                                                Text Annotation  oh_label  \n",
      "0  @halalflaws @biebervalue @greenlinerzjm I read...       none       0.0  \n",
      "1  @ShreyaBafna3 Now you idiots claim that people...       none       0.0  \n",
      "2  RT @Mooseoftorment Call me sexist, but when I ...     sexism       1.0  \n",
      "3  @g0ssipsquirrelx Wrong, ISIS follows the examp...     racism       1.0  \n",
      "4                             #mkr No No No No No No       none       0.0  \n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Processing File: toxicity_parsed_dataset.csv ===\n",
      "Column Names:\n",
      "['index', 'Text', 'ed_label_0', 'ed_label_1', 'oh_label']\n",
      "\n",
      "Data Types:\n",
      "index           int64\n",
      "Text           object\n",
      "ed_label_0    float64\n",
      "ed_label_1    float64\n",
      "oh_label        int64\n",
      "dtype: object\n",
      "\n",
      "Number of Rows: 159686\n",
      "Number of Columns: 5\n",
      "\n",
      "First 5 Rows:\n",
      "   index                                               Text  ed_label_0  \\\n",
      "0      0  This: :One can make an analogy in mathematical...         0.9   \n",
      "1      1  `  :Clarification for you  (and Zundark's righ...         1.0   \n",
      "2      2                          Elected or Electoral? JHK         1.0   \n",
      "3      3  `This is such a fun entry.   Devotchka  I once...         1.0   \n",
      "4      4  Please relate the ozone hole to increases in c...         0.8   \n",
      "\n",
      "   ed_label_1  oh_label  \n",
      "0         0.1         0  \n",
      "1         0.0         0  \n",
      "2         0.0         0  \n",
      "3         0.0         0  \n",
      "4         0.2         0  \n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Processing File: youtube_parsed_dataset.csv ===\n",
      "Column Names:\n",
      "['index', 'UserIndex', 'Text', 'Number of Comments', 'Number of Subscribers', 'Membership Duration', 'Number of Uploads', 'Profanity in UserID', 'Age', 'oh_label']\n",
      "\n",
      "Data Types:\n",
      "index                     int64\n",
      "UserIndex                object\n",
      "Text                     object\n",
      "Number of Comments        int64\n",
      "Number of Subscribers     int64\n",
      "Membership Duration       int64\n",
      "Number of Uploads         int64\n",
      "Profanity in UserID       int64\n",
      "Age                       int64\n",
      "oh_label                  int64\n",
      "dtype: object\n",
      "\n",
      "Number of Rows: 3464\n",
      "Number of Columns: 10\n",
      "\n",
      "First 5 Rows:\n",
      "   index UserIndex                                               Text  \\\n",
      "0      0        X1  Does N.e.bodyelse Hear her Crazy ass Screamin ...   \n",
      "1      1        X2  There are so many things that are incorrect wi...   \n",
      "2      2        X3  3:26 hahah my boyfriend showed this song to me...   \n",
      "3      3     X2218  dick beyonce fuck y a ass hole you are truely ...   \n",
      "4      4        X5  DongHaeTaemin and Kai ;A; luhansehun and bacon...   \n",
      "\n",
      "   Number of Comments  Number of Subscribers  Membership Duration  \\\n",
      "0                  10                      1                    3   \n",
      "1                   3                      0                    6   \n",
      "2                   7                      0                    3   \n",
      "3                  34                      0                    3   \n",
      "4                  11                    173                    5   \n",
      "\n",
      "   Number of Uploads  Profanity in UserID  Age  oh_label  \n",
      "0                  3                    0   15         0  \n",
      "1                  5                    0   31         0  \n",
      "2                  5                    0   43         1  \n",
      "3                  5                    0   44         1  \n",
      "4                  5                    0   21         0  \n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Processing File: Aggressive_All.csv ===\n",
      "Column Names:\n",
      "['No.', 'Message']\n",
      "\n",
      "Data Types:\n",
      "No.         int64\n",
      "Message    object\n",
      "dtype: object\n",
      "\n",
      "Number of Rows: 118828\n",
      "Number of Columns: 2\n",
      "\n",
      "First 5 Rows:\n",
      "   No.                                            Message\n",
      "0    1  zhha Islam does nothing but freeze the status ...\n",
      "1    2                       You dont get out much do you\n",
      "2    3  MaxBlumenthal Campagnebds Blumenthal self prom...\n",
      "3    4  No silly it isnt ITS UR MOMS and might I say q...\n",
      "4    5  Yes there is even more rape in Muslim countrie...\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Processing File: Non_Aggressive_All.csv ===\n",
      "Column Names:\n",
      "['No.', 'Message']\n",
      "\n",
      "Data Types:\n",
      "No.         int64\n",
      "Message    object\n",
      "dtype: object\n",
      "\n",
      "Number of Rows: 118828\n",
      "Number of Columns: 2\n",
      "\n",
      "First 5 Rows:\n",
      "   No.                                            Message\n",
      "0    1  Libya casualty report French operations   You ...\n",
      "1    2  Just for the record that IP is blocked for  ho...\n",
      "2    3  Big Brother Australia   I see you have partial...\n",
      "3    4  WikipediaFeatured portal candidatesPortalOrgan...\n",
      "4    5  wiki cant edit   could you make Thai airways s...\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# List of CSV file names present in the folder.\n",
    "csv_file_list = [\n",
    "    \"aggression_parsed_dataset.csv\",\n",
    "    \"attack_parsed_dataset.csv\",\n",
    "    \"Cyberbullying_Dataset_Summary_Table__Detailed_.csv\",\n",
    "    \"kaggle_parsed_dataset.csv\",\n",
    "    \"twitter_sexism_parsed_dataset.csv\",\n",
    "    \"twitter_racism_parsed_dataset.csv\",\n",
    "    \"twitter_parsed_dataset.csv\",\n",
    "    \"toxicity_parsed_dataset.csv\",\n",
    "    \"youtube_parsed_dataset.csv\",\n",
    "    \"Aggressive_All.csv\",\n",
    "    \"Non_Aggressive_All.csv\"\n",
    "\n",
    "]\n",
    "\n",
    "# Loop through each CSV file in the list.\n",
    "for file_name in csv_file_list:\n",
    "    # Since the CSV files are in the same folder as the notebook,\n",
    "    # the file path is simply the file name.\n",
    "    file_path = os.path.join(file_name)\n",
    "    \n",
    "    # Print header for current file processing.\n",
    "    print(f\"=== Processing File: {file_name} ===\")\n",
    "    \n",
    "    # Try to read the CSV file into a pandas DataFrame.\n",
    "    try:\n",
    "        data_frame = pd.read_csv(file_path)\n",
    "    except Exception as error:\n",
    "        print(f\"Error reading {file_name}: {error}\")\n",
    "        print(\"-\" * 60 + \"\\n\")\n",
    "        continue  # Skip to the next file if an error occurs.\n",
    "    \n",
    "    # Display the column names of the DataFrame.\n",
    "    print(\"Column Names:\")\n",
    "    print(data_frame.columns.tolist())\n",
    "    \n",
    "    # Display the data types of each column.\n",
    "    print(\"\\nData Types:\")\n",
    "    print(data_frame.dtypes)\n",
    "    \n",
    "    # Display the number of rows and columns in the DataFrame.\n",
    "    num_rows, num_columns = data_frame.shape\n",
    "    print(f\"\\nNumber of Rows: {num_rows}\")\n",
    "    print(f\"Number of Columns: {num_columns}\")\n",
    "    \n",
    "    # Display the first 5 rows of the DataFrame as a preview.\n",
    "    print(\"\\nFirst 5 Rows:\")\n",
    "    print(data_frame.head())\n",
    "    \n",
    "    # Print a separator line after processing each file.\n",
    "    print(\"\\n\" + \"-\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\t‚Ä¢\tLoads the CSV: Reads your CSV file into a DataFrame.\n",
    "\t‚Ä¢\tChecks for Duplicates: Counts and prints the number of duplicate rows, then removes them if found.\n",
    "\t‚Ä¢\tChecks for Empty Rows: Counts and prints rows that are completely empty (all values missing) and removes them.\n",
    "\t‚Ä¢\tSaves Cleaned Data: Writes the cleaned DataFrame to a new CSV file, preserving the changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the file name (update with your file name)\n",
    "file_name = 'your_file.csv' #put the file we want to check \n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "# Check for duplicate rows\n",
    "num_duplicates = df.duplicated().sum()\n",
    "print(f\"Found {num_duplicates} duplicate rows.\")\n",
    "\n",
    "# Remove duplicate rows, if any\n",
    "if num_duplicates > 0:\n",
    "    df = df.drop_duplicates()\n",
    "    print(\"Duplicate rows have been removed.\")\n",
    "\n",
    "# Check for empty rows (rows where all cells are NaN)\n",
    "num_empty_rows = df.isnull().all(axis=1).sum()\n",
    "print(f\"Found {num_empty_rows} empty rows.\")\n",
    "\n",
    "# Remove empty rows, if any\n",
    "if num_empty_rows > 0:\n",
    "    df = df.dropna(how='all')\n",
    "    print(\"Empty rows have been removed.\")\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "clean_file_name = 'your_file_cleaned.csv'# \n",
    "df.to_csv(clean_file_name, index=False)\n",
    "print(f\"Cleaned data has been saved to {clean_file_name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview of the Process\n",
    "\n",
    "In our pipeline, we began by loading and preprocessing a cleaned CSV file containing messages. We removed any rows with empty messages and standardized the text by converting it to lowercase and stripping extra whitespace. This ensured that our subsequent analysis was performed on a consistent and high-quality dataset.\n",
    "\n",
    "Emotion Analysis Approaches\n",
    "\n",
    "We employed two distinct emotion analysis methods. First, we used a lexicon-based approach with NRCLex. NRCLex utilizes the NRC Emotion Lexicon, which maps words to basic emotions such as anger, fear, joy, sadness, and disgust. By analyzing the frequency of these emotion-related words in a message, NRCLex returns a list of top emotions with their associated scores. This method is fast and interpretable, providing a straightforward snapshot of the emotional cues in the text.\n",
    "\n",
    "In addition, we applied a transformer-based approach using a pretrained model, specifically ‚Äúbhadresh-savani/distilbert-base-uncased-emotion‚Äù. This model is a distilled version of BERT that has been fine-tuned for emotion classification. It predicts multiple emotion categories‚Äîsuch as anger, joy, sadness, fear, love, and surprise‚Äîby returning a probability distribution over these labels for each message. We chose this model because DistilBERT is lighter and faster than the full BERT model while maintaining strong performance, and because it offers a detailed, probability-based classification that can capture nuanced emotional content.\n",
    "\n",
    "Creating the Trigger Column\n",
    "\n",
    "After obtaining the transformer-based emotion predictions, we created an additional column labeled ‚Äútrigger.‚Äù This column identifies the dominant emotion for each message by selecting the emotion with the highest probability from the transformer‚Äôs output. This trigger word serves as an immediate indicator of the primary emotional signal in a message, which can be very useful for further analysis or for developing interactive tools such as quizzes aimed at cyber bullying prevention.\n",
    "\n",
    "Saving the Results\n",
    "\n",
    "Finally, the enriched DataFrame‚Äînow containing the original messages, NRCLex emotion outputs, transformer-based emotion probabilities, and the trigger column‚Äîis saved in both CSV and JSON formats. The CSV file offers ease of use for further data manipulation or viewing in spreadsheet applications, while the JSON format is ideal for integration with web-based applications and interactive educational tools.\n",
    "\n",
    "By combining these methods, we achieve a comprehensive understanding of the emotional content in each message, enabling more informed analysis and effective strategies for cyber bullying prevention and educational initiatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nrclex import NRCLex\n",
    "from transformers import pipeline\n",
    "import textwrap\n",
    "import torch\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "import logging\n",
    "\n",
    "# Set logging level for transformers to suppress informational messages\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "# Verify PyTorch installation\n",
    "print(\"Installed PyTorch version:\", torch.__version__)\n",
    "print(\"Number of GPUs available:\", torch.cuda.device_count())\n",
    "\n",
    "# Download necessary NLTK data (if needed)\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# ------------------- Load and Preprocess Data -------------------\n",
    "file_name = \"Aggressive_All_cleaned.csv\"\n",
    "df = pd.read_csv(file_name)\n",
    "print(\"Columns in dataset:\", df.columns.tolist())\n",
    "\n",
    "# Remove rows where 'Message' is missing or empty\n",
    "df = df[~(df['Message'].isnull() | (df['Message'].astype(str).str.strip() == \"\"))]\n",
    "print(\"Number of rows after cleaning empty messages:\", len(df))\n",
    "\n",
    "# Standardize messages: convert to lowercase and strip extra whitespace\n",
    "df['Message'] = df['Message'].astype(str).str.lower().str.strip()\n",
    "\n",
    "# ------------------- Define Emotion Analysis Functions -------------------\n",
    "\n",
    "def get_nrc_emotions(text):\n",
    "    \"\"\"\n",
    "    Uses NRCLex to analyze the text and returns the top emotions as a list of (emotion, score) tuples.\n",
    "    \"\"\"\n",
    "    emotion_obj = NRCLex(text)\n",
    "    return emotion_obj.top_emotions\n",
    "\n",
    "def convert_transformer_results(results):\n",
    "    \"\"\"Convert a list of dictionaries to a single dictionary mapping emotion labels to scores.\"\"\"\n",
    "    return {item['label']: item['score'] for item in results}\n",
    "\n",
    "def process_batch(batch):\n",
    "    \"\"\"\n",
    "    Processes a batch of messages using the transformer pipeline.\n",
    "    Each process initializes its own pipeline instance.\n",
    "    \"\"\"\n",
    "    # Initialize the pipeline inside each process using CPU (change device=0 if using GPU)\n",
    "    emotion_classifier = pipeline(\n",
    "        \"text-classification\", \n",
    "        model=\"bhadresh-savani/distilbert-base-uncased-emotion\", \n",
    "        framework=\"pt\",        # explicitly use PyTorch\n",
    "        device=-1,             # use CPU; change to device=0 if GPU is available\n",
    "        top_k=None,            # equivalent to return_all_scores=True\n",
    "        truncation=True        # truncate texts longer than model's max length\n",
    "    )\n",
    "    results = emotion_classifier(batch)\n",
    "    # Convert each result (a list of dictionaries) into a single dictionary\n",
    "    processed = [convert_transformer_results(result) for result in results]\n",
    "    return processed\n",
    "\n",
    "# ------------------- Parallel Processing for Transformer Emotions -------------------\n",
    "def main():\n",
    "    # For faster experimentation, you can sample a subset (remove or adjust sampling as needed)\n",
    "    sample_size = 5000  # change or remove to process full dataset\n",
    "    df_sample = df.sample(n=sample_size, random_state=42)\n",
    "    messages = df_sample['Message'].tolist()\n",
    "    \n",
    "    batch_size = 32  # adjust based on your hardware\n",
    "    batches = [messages[i:i+batch_size] for i in range(0, len(messages), batch_size)]\n",
    "    \n",
    "    print(f\"Total messages in sample: {len(messages)}\")\n",
    "    print(f\"Total batches (batch size={batch_size}): {len(batches)}\")\n",
    "    \n",
    "    # Use multiprocessing Pool to process batches in parallel\n",
    "    with mp.Pool(mp.cpu_count()) as pool:\n",
    "        transformer_results = pool.map(process_batch, batches)\n",
    "    \n",
    "    # Flatten the list of lists into a single list for each message's transformer predictions\n",
    "    transformer_emotions = [item for sublist in transformer_results for item in sublist]\n",
    "    df_sample['transformer_emotions'] = transformer_emotions\n",
    "    \n",
    "    # Process NRCLex emotions (this step remains sequential)\n",
    "    print(\"Processing NRCLex emotions...\")\n",
    "    df_sample['nrc_emotions'] = df_sample['Message'].apply(get_nrc_emotions)\n",
    "    \n",
    "    # ------------------- Add 'trigger' Column -------------------\n",
    "    # For each row, pick the emotion from transformer_emotions with the highest probability.\n",
    "    df_sample['trigger'] = df_sample['transformer_emotions'].apply(\n",
    "        lambda x: max(x, key=x.get) if isinstance(x, dict) and len(x) > 0 else None\n",
    "    )\n",
    "    \n",
    "    # ------------------- Display Sample Results -------------------\n",
    "    print(\"\\n=== Sample Data with Emotion Predictions and Trigger Word ===\")\n",
    "    sample_display = df_sample[['Message', 'nrc_emotions', 'transformer_emotions', 'trigger']].head(5)\n",
    "    for idx, row in sample_display.iterrows():\n",
    "        print(\"-\" * 80)\n",
    "        print(\"Message:\")\n",
    "        print(textwrap.fill(row['Message'], width=80))\n",
    "        print(\"\\nNRCLex Top Emotions:\")\n",
    "        print(row['nrc_emotions'])\n",
    "        print(\"\\nTransformer-Based Emotions:\")\n",
    "        print(row['transformer_emotions'])\n",
    "        print(\"\\nTrigger Emotion (Highest Probability):\")\n",
    "        print(row['trigger'])\n",
    "        print(\"-\" * 80 + \"\\n\")\n",
    "    \n",
    "    # ------------------- Save the Updated DataFrame -------------------\n",
    "    csv_output = \"Aggressive_All_with_trigger.csv\"\n",
    "    json_output = \"Aggressive_All_with_trigger.json\"\n",
    "    \n",
    "    df_sample.to_csv(csv_output, index=False)\n",
    "    df_sample.to_json(json_output, orient='records', lines=True)\n",
    "    \n",
    "    print(\"CSV file saved as:\", csv_output)\n",
    "    print(\"JSON file saved as:\", json_output)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMPORARY: bypass SSL error (if it still exists)\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "# TEMPORARY: bypass SSL error (if it still exists)\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "self assesment tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from transformers import pipeline\n",
    "from nrclex import NRCLex\n",
    "import textwrap\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data (including punkt and punkt_tab)\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "# Download necessary NLTK data (if not already downloaded)\n",
    "nltk.download('vader_lexicon')  # optional; not needed for this example\n",
    "\n",
    "# Initialize the transformer-based emotion classifier.\n",
    "# Use device=0 if you have a GPU; here we assume CPU with device=-1.\n",
    "emotion_classifier = pipeline(\n",
    "    \"text-classification\", \n",
    "    model=\"bhadresh-savani/distilbert-base-uncased-emotion\", \n",
    "    framework=\"pt\",        # explicitly use PyTorch\n",
    "    device=-1,             # use CPU; change to device=0 for GPU\n",
    "    top_k=None,            # returns all scores\n",
    "    truncation=True        # truncates texts longer than model's max length\n",
    ")\n",
    "\n",
    "def get_transformer_emotions(text):\n",
    "    \"\"\"\n",
    "    Returns a dictionary mapping emotion labels to their probabilities using the transformer model.\n",
    "    \"\"\"\n",
    "    results = emotion_classifier(text)\n",
    "    return {item['label']: item['score'] for item in results[0]}\n",
    "\n",
    "def get_nrc_emotions(text):\n",
    "    \"\"\"\n",
    "    Returns NRCLex top emotions as a list of (emotion, score) tuples.\n",
    "    \"\"\"\n",
    "    emotion_obj = NRCLex(text)\n",
    "    return emotion_obj.top_emotions\n",
    "\n",
    "def main():\n",
    "    print(\"Welcome to the Emotion Trigger Questionnaire!\")\n",
    "    print(\"Type your statement below (or type 'quit' to exit):\\n\")\n",
    "    \n",
    "    while True:\n",
    "        # Get user input\n",
    "        statement = input(\"Enter your statement: \").strip()\n",
    "        if statement.lower() == 'quit':\n",
    "            print(\"Exiting the questionnaire. Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        # Process the statement using transformer and NRCLex\n",
    "        transformer_results = get_transformer_emotions(statement)\n",
    "        nrc_results = get_nrc_emotions(statement)\n",
    "        # Determine the trigger emotion from transformer results (emotion with highest probability)\n",
    "        trigger_emotion = max(transformer_results, key=transformer_results.get)\n",
    "        \n",
    "        # Display the results\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        print(\"Your Statement:\")\n",
    "        print(textwrap.fill(statement, width=80))\n",
    "        print(\"\\nTransformer-Based Emotion Scores:\")\n",
    "        for label, score in transformer_results.items():\n",
    "            print(f\"  {label}: {score:.3f}\")\n",
    "        print(\"\\nNRCLex Top Emotions:\")\n",
    "        print(nrc_results)\n",
    "        print(\"\\nTrigger Emotion (Highest Transformer Score):\", trigger_emotion)\n",
    "        print(\"-\" * 80 + \"\\n\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from transformers import pipeline\n",
    "from nrclex import NRCLex\n",
    "import textwrap\n",
    "import csv\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Initialize the transformer-based emotion classifier (using CPU here)\n",
    "emotion_classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"bhadresh-savani/distilbert-base-uncased-emotion\",\n",
    "    framework=\"pt\",\n",
    "    device=-1,\n",
    "    top_k=None,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "def get_transformer_emotions(text):\n",
    "    \"\"\"Returns a dictionary mapping emotion labels to probabilities using the transformer model.\"\"\"\n",
    "    results = emotion_classifier(text)\n",
    "    return {item['label']: item['score'] for item in results[0]}\n",
    "\n",
    "def get_nrc_emotions(text):\n",
    "    \"\"\"Uses NRCLex to analyze the text and returns top emotions as a list of (emotion, score) tuples.\"\"\"\n",
    "    emotion_obj = NRCLex(text)\n",
    "    return emotion_obj.top_emotions\n",
    "\n",
    "def interactive_survey():\n",
    "    \"\"\"Collects survey responses interactively from the user and returns a list of response records.\"\"\"\n",
    "    print(\"Welcome to the Cyber Bullying Self-Assessment Survey!\")\n",
    "    print(\"You will be asked to input examples of online messages and answer follow-up questions about how they make you feel.\")\n",
    "    print(\"Type 'quit' at any prompt to exit.\\n\")\n",
    "    \n",
    "    responses = []\n",
    "    \n",
    "    while True:\n",
    "        message = input(\"Enter an online message: \").strip()\n",
    "        if message.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        # Get emotion analyses\n",
    "        transformer_emotions = get_transformer_emotions(message)\n",
    "        nrc_emotions = get_nrc_emotions(message)\n",
    "        trigger_emotion = max(transformer_emotions, key=transformer_emotions.get)\n",
    "        \n",
    "        # Ask follow-up questions\n",
    "        user_feeling = input(\"1. How does this message make you feel? \").strip()\n",
    "        if user_feeling.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        intensity = input(\"2. On a scale of 1 (low) to 10 (high), how intense is that feeling? \").strip()\n",
    "        if intensity.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        mood_impact = input(\"3. Does this message affect your mood? (yes/no): \").strip()\n",
    "        if mood_impact.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        response_action = input(\"4. What would you do if you encountered this message in real life? \").strip()\n",
    "        if response_action.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        # Store the response in a dictionary record\n",
    "        record = {\n",
    "            \"message\": message,\n",
    "            \"transformer_emotions\": transformer_emotions,\n",
    "            \"nrc_emotions\": nrc_emotions,\n",
    "            \"trigger\": trigger_emotion,\n",
    "            \"user_feeling\": user_feeling,\n",
    "            \"intensity\": intensity,\n",
    "            \"mood_impact\": mood_impact,\n",
    "            \"response_action\": response_action\n",
    "        }\n",
    "        responses.append(record)\n",
    "        print(\"\\nResponse recorded!\\n\")\n",
    "        cont = input(\"Would you like to assess another message? (yes/no): \").strip().lower()\n",
    "        if cont != \"yes\":\n",
    "            break\n",
    "    \n",
    "    return responses\n",
    "\n",
    "def save_responses_to_csv(responses, filename=\"survey_responses.csv\"):\n",
    "    \"\"\"Saves a list of response records to a CSV file.\"\"\"\n",
    "    if not responses:\n",
    "        print(\"No responses to save.\")\n",
    "        return\n",
    "    # Get the header from keys of the first record.\n",
    "    header = responses[0].keys()\n",
    "    with open(filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=header)\n",
    "        writer.writeheader()\n",
    "        for record in responses:\n",
    "            writer.writerow(record)\n",
    "    print(f\"Responses saved to {filename}\")\n",
    "\n",
    "# Run the interactive survey and save responses to CSV.\n",
    "if __name__ == \"__main__\":\n",
    "    survey_responses = interactive_survey()\n",
    "    save_responses_to_csv(survey_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################SURVEY AND MODEL \n",
    "import nltk\n",
    "from transformers import pipeline\n",
    "from nrclex import NRCLex\n",
    "import textwrap\n",
    "import csv\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Initialize the transformer-based emotion classifier (using CPU here)\n",
    "emotion_classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"bhadresh-savani/distilbert-base-uncased-emotion\",\n",
    "    framework=\"pt\",\n",
    "    device=-1,\n",
    "    top_k=None,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "def get_transformer_emotions(text):\n",
    "    \"\"\"Returns a dictionary mapping emotion labels to probabilities using the transformer model.\"\"\"\n",
    "    results = emotion_classifier(text)\n",
    "    return {item['label']: item['score'] for item in results[0]}\n",
    "\n",
    "def get_nrc_emotions(text):\n",
    "    \"\"\"Uses NRCLex to analyze the text and returns top emotions as a list of (emotion, score) tuples.\"\"\"\n",
    "    emotion_obj = NRCLex(text)\n",
    "    return emotion_obj.top_emotions\n",
    "\n",
    "def interactive_survey():\n",
    "    \"\"\"Collects survey responses interactively from the user and returns a list of response records.\"\"\"\n",
    "    print(\"Welcome to the Cyber Bullying Self-Assessment Survey!\")\n",
    "    print(\"You will be asked to input examples of online messages and answer follow-up questions about how they make you feel.\")\n",
    "    print(\"Type 'quit' at any prompt to exit.\\n\")\n",
    "    \n",
    "    responses = []\n",
    "    \n",
    "    while True:\n",
    "        message = input(\"Enter an online message: \").strip()\n",
    "        if message.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        # Get emotion analyses\n",
    "        transformer_emotions = get_transformer_emotions(message)\n",
    "        nrc_emotions = get_nrc_emotions(message)\n",
    "        trigger_emotion = max(transformer_emotions, key=transformer_emotions.get)\n",
    "        \n",
    "        # Ask follow-up questions\n",
    "        user_feeling = input(\"1. How does this message make you feel? \").strip()\n",
    "        if user_feeling.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        intensity = input(\"2. On a scale of 1 (low) to 10 (high), how intense is that feeling? \").strip()\n",
    "        if intensity.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        mood_impact = input(\"3. Does this message affect your mood? (yes/no): \").strip()\n",
    "        if mood_impact.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        response_action = input(\"4. What would you do if you encountered this message in real life? \").strip()\n",
    "        if response_action.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        # Store the response in a dictionary record\n",
    "        record = {\n",
    "            \"message\": message,\n",
    "            \"transformer_emotions\": transformer_emotions,\n",
    "            \"nrc_emotions\": nrc_emotions,\n",
    "            \"trigger\": trigger_emotion,\n",
    "            \"user_feeling\": user_feeling,\n",
    "            \"intensity\": intensity,\n",
    "            \"mood_impact\": mood_impact,\n",
    "            \"response_action\": response_action\n",
    "        }\n",
    "        responses.append(record)\n",
    "        print(\"\\nResponse recorded!\\n\")\n",
    "        cont = input(\"Would you like to assess another message? (yes/no): \").strip().lower()\n",
    "        if cont != \"yes\":\n",
    "            break\n",
    "    \n",
    "    return responses\n",
    "\n",
    "def save_responses_to_csv(responses, filename=\"survey_responses.csv\"):\n",
    "    \"\"\"Saves a list of response records to a CSV file.\"\"\"\n",
    "    if not responses:\n",
    "        print(\"No responses to save.\")\n",
    "        return\n",
    "    # Get the header from keys of the first record.\n",
    "    header = responses[0].keys()\n",
    "    with open(filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=header)\n",
    "        writer.writeheader()\n",
    "        for record in responses:\n",
    "            writer.writerow(record)\n",
    "    print(f\"Responses saved to {filename}\")\n",
    "\n",
    "# Run the interactive survey and save responses to CSV.\n",
    "if __name__ == \"__main__\":\n",
    "    survey_responses = interactive_survey()\n",
    "    save_responses_to_csv(survey_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################## SURVEY PLOT\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the survey responses CSV file\n",
    "df = pd.read_csv(\"survey_responses.csv\")\n",
    "\n",
    "# Inspect the columns\n",
    "print(\"Columns in survey data:\", df.columns.tolist())\n",
    "\n",
    "# For trend analysis, we'll use:\n",
    "# - 'trigger' (categorical trigger emotion)\n",
    "# - 'intensity' (convert to numeric)\n",
    "# - 'mood_impact' (convert yes/no to 1/0)\n",
    "df['intensity'] = pd.to_numeric(df['intensity'], errors='coerce')\n",
    "df['mood_impact_numeric'] = df['mood_impact'].apply(lambda x: 1 if x.strip().lower() == 'yes' else 0)\n",
    "\n",
    "# 1. Trend Analysis\n",
    "# a) Frequency of trigger emotions\n",
    "trigger_counts = df['trigger'].value_counts()\n",
    "print(\"\\nTrigger Emotion Frequencies:\")\n",
    "print(trigger_counts)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "trigger_counts.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "plt.title(\"Frequency of Trigger Emotions\")\n",
    "plt.xlabel(\"Trigger Emotion\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# b) Average intensity by trigger emotion\n",
    "avg_intensity = df.groupby('trigger')['intensity'].mean()\n",
    "print(\"\\nAverage Intensity by Trigger Emotion:\")\n",
    "print(avg_intensity)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "avg_intensity.plot(kind='bar', color='coral', edgecolor='black')\n",
    "plt.title(\"Average Intensity by Trigger Emotion\")\n",
    "plt.xlabel(\"Trigger Emotion\")\n",
    "plt.ylabel(\"Average Intensity (1-10)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Risk Identification via Clustering\n",
    "# We create a feature matrix using 'intensity' and 'mood_impact_numeric'\n",
    "features = df[['intensity', 'mood_impact_numeric']].dropna()\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Determine number of clusters (e.g., 3 clusters)\n",
    "k = 3\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "clusters = kmeans.fit_predict(features_scaled)\n",
    "\n",
    "# Add cluster assignments to the DataFrame\n",
    "df.loc[features.index, 'cluster'] = clusters\n",
    "\n",
    "print(\"\\nCluster Counts:\")\n",
    "print(df['cluster'].value_counts())\n",
    "\n",
    "# Visualize clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(features_scaled[:, 0], features_scaled[:, 1], c=clusters, cmap='viridis', alpha=0.6)\n",
    "plt.title(\"Clustering of Users by Intensity and Mood Impact\")\n",
    "plt.xlabel(\"Standardized Intensity\")\n",
    "plt.ylabel(\"Standardized Mood Impact\")\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Output insights\n",
    "print(\"\\nData-Driven Insights:\")\n",
    "print(\"Trigger Emotion Frequencies:\\n\", trigger_counts)\n",
    "print(\"Average Intensity by Trigger Emotion:\\n\", avg_intensity)\n",
    "print(\"Cluster assignments:\\n\", df['cluster'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
