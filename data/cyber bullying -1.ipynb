{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color: #FF5733; font-size: 56px;\"> Cyber Bullying prevention</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color: pink; font-size: 36px;\">Digital Defender: Game Wrangling and Analysis File</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color: green; font-size: 19px;\"> Data details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script uses the os and pandas libraries to manage and process multiple CSV files stored in the same directory. It iterates over a list of CSV filenames, reads each file into a pandas DataFrame, and then prints essential details such as column names, data types, the shape of the DataFrame, and a preview of the first few rows. If a file can’t be read due to an error, the code prints an error message and continues with the next file, ensuring a clear separation between the outputs of each file. This setup provides a quick and efficient way to explore and understand the structure and content of your CSV data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Processing File: aggression_parsed_dataset.csv ===\n",
      "Column Names:\n",
      "['index', 'Text', 'ed_label_0', 'ed_label_1', 'oh_label']\n",
      "\n",
      "Data Types:\n",
      "index           int64\n",
      "Text           object\n",
      "ed_label_0    float64\n",
      "ed_label_1    float64\n",
      "oh_label        int64\n",
      "dtype: object\n",
      "\n",
      "Number of Rows: 115864\n",
      "Number of Columns: 5\n",
      "\n",
      "First 5 Rows:\n",
      "   index                                               Text  ed_label_0  \\\n",
      "0      0  `- This is not ``creative``.  Those are the di...    0.900000   \n",
      "1      1  `  :: the term ``standard model`` is itself le...    1.000000   \n",
      "2      2    True or false, the situation as of March 200...    1.000000   \n",
      "3      3   Next, maybe you could work on being less cond...    0.555556   \n",
      "4      4               This page will need disambiguation.     1.000000   \n",
      "\n",
      "   ed_label_1  oh_label  \n",
      "0    0.100000         0  \n",
      "1    0.000000         0  \n",
      "2    0.000000         0  \n",
      "3    0.444444         0  \n",
      "4    0.000000         0  \n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Processing File: attack_parsed_dataset.csv ===\n",
      "Column Names:\n",
      "['index', 'Text', 'ed_label_0', 'ed_label_1', 'oh_label']\n",
      "\n",
      "Data Types:\n",
      "index           int64\n",
      "Text           object\n",
      "ed_label_0    float64\n",
      "ed_label_1    float64\n",
      "oh_label        int64\n",
      "dtype: object\n",
      "\n",
      "Number of Rows: 115864\n",
      "Number of Columns: 5\n",
      "\n",
      "First 5 Rows:\n",
      "   index                                               Text  ed_label_0  \\\n",
      "0      0  `- This is not ``creative``.  Those are the di...    1.000000   \n",
      "1      1  `  :: the term ``standard model`` is itself le...    1.000000   \n",
      "2      2    True or false, the situation as of March 200...    1.000000   \n",
      "3      3   Next, maybe you could work on being less cond...    0.555556   \n",
      "4      4               This page will need disambiguation.     1.000000   \n",
      "\n",
      "   ed_label_1  oh_label  \n",
      "0    0.000000         0  \n",
      "1    0.000000         0  \n",
      "2    0.000000         0  \n",
      "3    0.444444         0  \n",
      "4    0.000000         0  \n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Processing File: Cyberbullying_Dataset_Summary_Table__Detailed_.csv ===\n",
      "Column Names:\n",
      "['Names', 'File Type', 'Link address', 'Description']\n",
      "\n",
      "Data Types:\n",
      "Names           object\n",
      "File Type       object\n",
      "Link address    object\n",
      "Description     object\n",
      "dtype: object\n",
      "\n",
      "Number of Rows: 8\n",
      "Number of Columns: 4\n",
      "\n",
      "First 5 Rows:\n",
      "                Names File Type           Link address  \\\n",
      "0  Aggression dataset       CSV  User uploaded locally   \n",
      "1      Attack dataset       CSV  User uploaded locally   \n",
      "2    Toxicity dataset       CSV  User uploaded locally   \n",
      "3      Kaggle dataset       CSV  User uploaded locally   \n",
      "4     Twitter dataset       CSV  User uploaded locally   \n",
      "\n",
      "                                         Description  \n",
      "0  Contains user-generated comments along with ma...  \n",
      "1  Similar to the aggression dataset, this file c...  \n",
      "2  This dataset is geared toward detecting toxic ...  \n",
      "3  A streamlined dataset from Kaggle containing t...  \n",
      "4  This dataset includes text messages with annot...  \n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Processing File: kaggle_parsed_dataset.csv ===\n",
      "Column Names:\n",
      "['index', 'oh_label', 'Date', 'Text']\n",
      "\n",
      "Data Types:\n",
      "index        int64\n",
      "oh_label     int64\n",
      "Date        object\n",
      "Text        object\n",
      "dtype: object\n",
      "\n",
      "Number of Rows: 8799\n",
      "Number of Columns: 4\n",
      "\n",
      "First 5 Rows:\n",
      "   index  oh_label             Date  \\\n",
      "0      0         1  20120618192155Z   \n",
      "1      1         0  20120528192215Z   \n",
      "2      2         0              NaN   \n",
      "3      3         0              NaN   \n",
      "4      4         0  20120619094753Z   \n",
      "\n",
      "                                                Text  \n",
      "0                               \"You fuck your dad.\"  \n",
      "1  \"i really don't understand your point.\\xa0 It ...  \n",
      "2  \"A\\\\xc2\\\\xa0majority of Canadians can and has ...  \n",
      "3  \"listen if you dont wanna get married to a man...  \n",
      "4  \"C\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1edd...  \n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Processing File: twitter_sexism_parsed_dataset.csv ===\n",
      "Column Names:\n",
      "['index', 'id', 'Text', 'Annotation', 'oh_label']\n",
      "\n",
      "Data Types:\n",
      "index          object\n",
      "id             object\n",
      "Text           object\n",
      "Annotation     object\n",
      "oh_label      float64\n",
      "dtype: object\n",
      "\n",
      "Number of Rows: 14881\n",
      "Number of Columns: 5\n",
      "\n",
      "First 5 Rows:\n",
      "                   index                     id  \\\n",
      "0  5.35198627292254E+017  5.35198627292254E+017   \n",
      "1  5.75984924030714E+017  5.75984924030714E+017   \n",
      "2   5.7233536016588E+017   5.7233536016588E+017   \n",
      "3  5.72337925708374E+017  5.72337925708374E+017   \n",
      "4  4.43033024528011E+017  4.43033024528011E+017   \n",
      "\n",
      "                                                Text Annotation  oh_label  \n",
      "0  RT @BeepsS: @senna1 @BeepsS: I'm not sexist bu...     sexism       1.0  \n",
      "1   There's some very hate able teams this year #MKR       none       0.0  \n",
      "2  RT @The_Eccles: \"Everyone underestimated us\" \\...       none       0.0  \n",
      "3  RT @NOTLukeDarcy: did @Channel7 or #MKR actual...       none       0.0  \n",
      "4  No, you don't. @Shut_Up_Jeff: I thought of a r...     sexism       1.0  \n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Processing File: twitter_racism_parsed_dataset.csv ===\n",
      "Column Names:\n",
      "['index', 'id', 'Text', 'Annotation', 'oh_label']\n",
      "\n",
      "Data Types:\n",
      "index         float64\n",
      "id            float64\n",
      "Text           object\n",
      "Annotation     object\n",
      "oh_label        int64\n",
      "dtype: object\n",
      "\n",
      "Number of Rows: 13471\n",
      "Number of Columns: 5\n",
      "\n",
      "First 5 Rows:\n",
      "          index            id  \\\n",
      "0  5.767493e+17  5.767493e+17   \n",
      "1  5.408905e+17  5.408905e+17   \n",
      "2  5.678433e+17  5.678433e+17   \n",
      "3  5.766462e+17  5.766462e+17   \n",
      "4  5.713492e+17  5.713492e+17   \n",
      "\n",
      "                                                Text Annotation  oh_label  \n",
      "0  @AAlwuhaib1977 Muslim mob violence against Hin...     racism         1  \n",
      "1             @Te4m_NiGhtM4Re http://t.co/5Ih7MkDbQG       none         0  \n",
      "2  @jncatron @isra_jourisra @AMPalestine Islamoph...     racism         1  \n",
      "3  Finally I'm all caught up, and that sudden dea...       none         0  \n",
      "4             @carolinesinders @herecomesfran *hugs*       none         0  \n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Processing File: twitter_parsed_dataset.csv ===\n",
      "Column Names:\n",
      "['index', 'id', 'Text', 'Annotation', 'oh_label']\n",
      "\n",
      "Data Types:\n",
      "index          object\n",
      "id             object\n",
      "Text           object\n",
      "Annotation     object\n",
      "oh_label      float64\n",
      "dtype: object\n",
      "\n",
      "Number of Rows: 16851\n",
      "Number of Columns: 5\n",
      "\n",
      "First 5 Rows:\n",
      "                   index                     id  \\\n",
      "0  5.74948705591165E+017  5.74948705591165E+017   \n",
      "1  5.71917888690393E+017  5.71917888690393E+017   \n",
      "2  3.90255841338601E+017  3.90255841338601E+017   \n",
      "3  5.68208850655916E+017  5.68208850655916E+017   \n",
      "4  5.75596338802373E+017  5.75596338802373E+017   \n",
      "\n",
      "                                                Text Annotation  oh_label  \n",
      "0  @halalflaws @biebervalue @greenlinerzjm I read...       none       0.0  \n",
      "1  @ShreyaBafna3 Now you idiots claim that people...       none       0.0  \n",
      "2  RT @Mooseoftorment Call me sexist, but when I ...     sexism       1.0  \n",
      "3  @g0ssipsquirrelx Wrong, ISIS follows the examp...     racism       1.0  \n",
      "4                             #mkr No No No No No No       none       0.0  \n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Processing File: toxicity_parsed_dataset.csv ===\n",
      "Column Names:\n",
      "['index', 'Text', 'ed_label_0', 'ed_label_1', 'oh_label']\n",
      "\n",
      "Data Types:\n",
      "index           int64\n",
      "Text           object\n",
      "ed_label_0    float64\n",
      "ed_label_1    float64\n",
      "oh_label        int64\n",
      "dtype: object\n",
      "\n",
      "Number of Rows: 159686\n",
      "Number of Columns: 5\n",
      "\n",
      "First 5 Rows:\n",
      "   index                                               Text  ed_label_0  \\\n",
      "0      0  This: :One can make an analogy in mathematical...         0.9   \n",
      "1      1  `  :Clarification for you  (and Zundark's righ...         1.0   \n",
      "2      2                          Elected or Electoral? JHK         1.0   \n",
      "3      3  `This is such a fun entry.   Devotchka  I once...         1.0   \n",
      "4      4  Please relate the ozone hole to increases in c...         0.8   \n",
      "\n",
      "   ed_label_1  oh_label  \n",
      "0         0.1         0  \n",
      "1         0.0         0  \n",
      "2         0.0         0  \n",
      "3         0.0         0  \n",
      "4         0.2         0  \n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Processing File: youtube_parsed_dataset.csv ===\n",
      "Column Names:\n",
      "['index', 'UserIndex', 'Text', 'Number of Comments', 'Number of Subscribers', 'Membership Duration', 'Number of Uploads', 'Profanity in UserID', 'Age', 'oh_label']\n",
      "\n",
      "Data Types:\n",
      "index                     int64\n",
      "UserIndex                object\n",
      "Text                     object\n",
      "Number of Comments        int64\n",
      "Number of Subscribers     int64\n",
      "Membership Duration       int64\n",
      "Number of Uploads         int64\n",
      "Profanity in UserID       int64\n",
      "Age                       int64\n",
      "oh_label                  int64\n",
      "dtype: object\n",
      "\n",
      "Number of Rows: 3464\n",
      "Number of Columns: 10\n",
      "\n",
      "First 5 Rows:\n",
      "   index UserIndex                                               Text  \\\n",
      "0      0        X1  Does N.e.bodyelse Hear her Crazy ass Screamin ...   \n",
      "1      1        X2  There are so many things that are incorrect wi...   \n",
      "2      2        X3  3:26 hahah my boyfriend showed this song to me...   \n",
      "3      3     X2218  dick beyonce fuck y a ass hole you are truely ...   \n",
      "4      4        X5  DongHaeTaemin and Kai ;A; luhansehun and bacon...   \n",
      "\n",
      "   Number of Comments  Number of Subscribers  Membership Duration  \\\n",
      "0                  10                      1                    3   \n",
      "1                   3                      0                    6   \n",
      "2                   7                      0                    3   \n",
      "3                  34                      0                    3   \n",
      "4                  11                    173                    5   \n",
      "\n",
      "   Number of Uploads  Profanity in UserID  Age  oh_label  \n",
      "0                  3                    0   15         0  \n",
      "1                  5                    0   31         0  \n",
      "2                  5                    0   43         1  \n",
      "3                  5                    0   44         1  \n",
      "4                  5                    0   21         0  \n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Processing File: Aggressive_All.csv ===\n",
      "Column Names:\n",
      "['No.', 'Message']\n",
      "\n",
      "Data Types:\n",
      "No.         int64\n",
      "Message    object\n",
      "dtype: object\n",
      "\n",
      "Number of Rows: 118828\n",
      "Number of Columns: 2\n",
      "\n",
      "First 5 Rows:\n",
      "   No.                                            Message\n",
      "0    1  zhha Islam does nothing but freeze the status ...\n",
      "1    2                       You dont get out much do you\n",
      "2    3  MaxBlumenthal Campagnebds Blumenthal self prom...\n",
      "3    4  No silly it isnt ITS UR MOMS and might I say q...\n",
      "4    5  Yes there is even more rape in Muslim countrie...\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== Processing File: Non_Aggressive_All.csv ===\n",
      "Column Names:\n",
      "['No.', 'Message']\n",
      "\n",
      "Data Types:\n",
      "No.         int64\n",
      "Message    object\n",
      "dtype: object\n",
      "\n",
      "Number of Rows: 118828\n",
      "Number of Columns: 2\n",
      "\n",
      "First 5 Rows:\n",
      "   No.                                            Message\n",
      "0    1  Libya casualty report French operations   You ...\n",
      "1    2  Just for the record that IP is blocked for  ho...\n",
      "2    3  Big Brother Australia   I see you have partial...\n",
      "3    4  WikipediaFeatured portal candidatesPortalOrgan...\n",
      "4    5  wiki cant edit   could you make Thai airways s...\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# List of CSV file names present in the folder.\n",
    "csv_file_list = [\n",
    "    \"aggression_parsed_dataset.csv\",\n",
    "    \"attack_parsed_dataset.csv\",\n",
    "    \"Cyberbullying_Dataset_Summary_Table__Detailed_.csv\",\n",
    "    \"kaggle_parsed_dataset.csv\",\n",
    "    \"twitter_sexism_parsed_dataset.csv\",\n",
    "    \"twitter_racism_parsed_dataset.csv\",\n",
    "    \"twitter_parsed_dataset.csv\",\n",
    "    \"toxicity_parsed_dataset.csv\",\n",
    "    \"youtube_parsed_dataset.csv\",\n",
    "    \"Aggressive_All.csv\",\n",
    "    \"Non_Aggressive_All.csv\"\n",
    "\n",
    "]\n",
    "\n",
    "# Loop through each CSV file in the list.\n",
    "for file_name in csv_file_list:\n",
    "    # Since the CSV files are in the same folder as the notebook,\n",
    "    # the file path is simply the file name.\n",
    "    file_path = os.path.join(file_name)\n",
    "    \n",
    "    # Print header for current file processing.\n",
    "    print(f\"=== Processing File: {file_name} ===\")\n",
    "    \n",
    "    # Try to read the CSV file into a pandas DataFrame.\n",
    "    try:\n",
    "        data_frame = pd.read_csv(file_path)\n",
    "    except Exception as error:\n",
    "        print(f\"Error reading {file_name}: {error}\")\n",
    "        print(\"-\" * 60 + \"\\n\")\n",
    "        continue  # Skip to the next file if an error occurs.\n",
    "    \n",
    "    # Display the column names of the DataFrame.\n",
    "    print(\"Column Names:\")\n",
    "    print(data_frame.columns.tolist())\n",
    "    \n",
    "    # Display the data types of each column.\n",
    "    print(\"\\nData Types:\")\n",
    "    print(data_frame.dtypes)\n",
    "    \n",
    "    # Display the number of rows and columns in the DataFrame.\n",
    "    num_rows, num_columns = data_frame.shape\n",
    "    print(f\"\\nNumber of Rows: {num_rows}\")\n",
    "    print(f\"Number of Columns: {num_columns}\")\n",
    "    \n",
    "    # Display the first 5 rows of the DataFrame as a preview.\n",
    "    print(\"\\nFirst 5 Rows:\")\n",
    "    print(data_frame.head())\n",
    "    \n",
    "    # Print a separator line after processing each file.\n",
    "    print(\"\\n\" + \"-\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color: green; font-size: 19px;\"> Data wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script begins by loading the CSV file into a DataFrame, which serves as the initial dataset. It then checks for duplicate rows by counting and printing their number, and subsequently removes any duplicates found. In addition, the script identifies rows that are entirely empty—meaning all values are missing—counts and prints these empty rows, and removes them from the DataFrame. Finally, it saves the cleaned DataFrame to a new CSV file, ensuring that all modifications are preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the file name (update with your file name)\n",
    "file_name = 'your_file.csv' #put the file we want to check \n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "# Check for duplicate rows\n",
    "num_duplicates = df.duplicated().sum()\n",
    "print(f\"Found {num_duplicates} duplicate rows.\")\n",
    "\n",
    "# Remove duplicate rows, if any\n",
    "if num_duplicates > 0:\n",
    "    df = df.drop_duplicates()\n",
    "    print(\"Duplicate rows have been removed.\")\n",
    "\n",
    "# Check for empty rows (rows where all cells are NaN)\n",
    "num_empty_rows = df.isnull().all(axis=1).sum()\n",
    "print(f\"Found {num_empty_rows} empty rows.\")\n",
    "\n",
    "# Remove empty rows, if any\n",
    "if num_empty_rows > 0:\n",
    "    df = df.dropna(how='all')\n",
    "    print(\"Empty rows have been removed.\")\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "clean_file_name = 'your_file_cleaned.csv'# \n",
    "df.to_csv(clean_file_name, index=False)\n",
    "print(f\"Cleaned data has been saved to {clean_file_name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color: #FF5733; font-size: 19px;\">Natural language processing  and analysis in file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview of the Process\n",
    "\n",
    "In our pipeline, we began by loading and preprocessing a cleaned CSV file containing messages. We removed any rows with empty messages and standardized the text by converting it to lowercase and stripping extra whitespace. This ensured that our subsequent analysis was performed on a consistent and high-quality dataset.\n",
    "\n",
    "Emotion Analysis Approaches\n",
    "\n",
    "We employed two distinct emotion analysis methods. First, we used a lexicon-based approach with NRCLex. NRCLex utilizes the NRC Emotion Lexicon, which maps words to basic emotions such as anger, fear, joy, sadness, and disgust. By analyzing the frequency of these emotion-related words in a message, NRCLex returns a list of top emotions with their associated scores. This method is fast and interpretable, providing a straightforward snapshot of the emotional cues in the text.\n",
    "\n",
    "In addition, we applied a transformer-based approach using a pretrained model, specifically “bhadresh-savani/distilbert-base-uncased-emotion”. This model is a distilled version of BERT that has been fine-tuned for emotion classification. It predicts multiple emotion categories—such as anger, joy, sadness, fear, love, and surprise—by returning a probability distribution over these labels for each message. We chose this model because DistilBERT is lighter and faster than the full BERT model while maintaining strong performance, and because it offers a detailed, probability-based classification that can capture nuanced emotional content.\n",
    "\n",
    "Creating the Trigger Column\n",
    "\n",
    "After obtaining the transformer-based emotion predictions, we created an additional column labeled “trigger.” This column identifies the dominant emotion for each message by selecting the emotion with the highest probability from the transformer’s output. This trigger word serves as an immediate indicator of the primary emotional signal in a message, which can be very useful for further analysis or for developing interactive tools such as quizzes aimed at cyber bullying prevention.\n",
    "\n",
    "Saving the Results\n",
    "\n",
    "Finally, the enriched DataFrame—now containing the original messages, NRCLex emotion outputs, transformer-based emotion probabilities, and the trigger column—is saved in both CSV and JSON formats. The CSV file offers ease of use for further data manipulation or viewing in spreadsheet applications, while the JSON format is ideal for integration with web-based applications and interactive educational tools.\n",
    "\n",
    "By combining these methods, we achieve a comprehensive understanding of the emotional content in each message, enabling more informed analysis and effective strategies for cyber bullying prevention and educational initiatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nrclex import NRCLex\n",
    "from transformers import pipeline\n",
    "import textwrap\n",
    "import torch\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "import logging\n",
    "\n",
    "# Set logging level for transformers to suppress informational messages\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "# Verify PyTorch installation\n",
    "print(\"Installed PyTorch version:\", torch.__version__)\n",
    "print(\"Number of GPUs available:\", torch.cuda.device_count())\n",
    "\n",
    "# Download necessary NLTK data (if needed)\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# ------------------- Load and Preprocess Data -------------------\n",
    "file_name = \"Aggressive_All_cleaned.csv\"\n",
    "df = pd.read_csv(file_name)\n",
    "print(\"Columns in dataset:\", df.columns.tolist())\n",
    "\n",
    "# Remove rows where 'Message' is missing or empty\n",
    "df = df[~(df['Message'].isnull() | (df['Message'].astype(str).str.strip() == \"\"))]\n",
    "print(\"Number of rows after cleaning empty messages:\", len(df))\n",
    "\n",
    "# Standardize messages: convert to lowercase and strip extra whitespace\n",
    "df['Message'] = df['Message'].astype(str).str.lower().str.strip()\n",
    "\n",
    "# ------------------- Define Emotion Analysis Functions -------------------\n",
    "\n",
    "def get_nrc_emotions(text):\n",
    "    \"\"\"\n",
    "    Uses NRCLex to analyze the text and returns the top emotions as a list of (emotion, score) tuples.\n",
    "    \"\"\"\n",
    "    emotion_obj = NRCLex(text)\n",
    "    return emotion_obj.top_emotions\n",
    "\n",
    "def convert_transformer_results(results):\n",
    "    \"\"\"Convert a list of dictionaries to a single dictionary mapping emotion labels to scores.\"\"\"\n",
    "    return {item['label']: item['score'] for item in results}\n",
    "\n",
    "def process_batch(batch):\n",
    "    \"\"\"\n",
    "    Processes a batch of messages using the transformer pipeline.\n",
    "    Each process initializes its own pipeline instance.\n",
    "    \"\"\"\n",
    "    # Initialize the pipeline inside each process using CPU (change device=0 if using GPU)\n",
    "    emotion_classifier = pipeline(\n",
    "        \"text-classification\", \n",
    "        model=\"bhadresh-savani/distilbert-base-uncased-emotion\", \n",
    "        framework=\"pt\",        # explicitly use PyTorch\n",
    "        device=-1,             # use CPU; change to device=0 if GPU is available\n",
    "        top_k=None,            # equivalent to return_all_scores=True\n",
    "        truncation=True        # truncate texts longer than model's max length\n",
    "    )\n",
    "    results = emotion_classifier(batch)\n",
    "    # Convert each result (a list of dictionaries) into a single dictionary\n",
    "    processed = [convert_transformer_results(result) for result in results]\n",
    "    return processed\n",
    "\n",
    "# ------------------- Parallel Processing for Transformer Emotions -------------------\n",
    "def main():\n",
    "    # For faster experimentation, you can sample a subset (remove or adjust sampling as needed)\n",
    "    sample_size = 5000  # change or remove to process full dataset\n",
    "    df_sample = df.sample(n=sample_size, random_state=42)\n",
    "    messages = df_sample['Message'].tolist()\n",
    "    \n",
    "    batch_size = 32  # adjust based on your hardware\n",
    "    batches = [messages[i:i+batch_size] for i in range(0, len(messages), batch_size)]\n",
    "    \n",
    "    print(f\"Total messages in sample: {len(messages)}\")\n",
    "    print(f\"Total batches (batch size={batch_size}): {len(batches)}\")\n",
    "    \n",
    "    # Use multiprocessing Pool to process batches in parallel\n",
    "    with mp.Pool(mp.cpu_count()) as pool:\n",
    "        transformer_results = pool.map(process_batch, batches)\n",
    "    \n",
    "    # Flatten the list of lists into a single list for each message's transformer predictions\n",
    "    transformer_emotions = [item for sublist in transformer_results for item in sublist]\n",
    "    df_sample['transformer_emotions'] = transformer_emotions\n",
    "    \n",
    "    # Process NRCLex emotions (this step remains sequential)\n",
    "    print(\"Processing NRCLex emotions...\")\n",
    "    df_sample['nrc_emotions'] = df_sample['Message'].apply(get_nrc_emotions)\n",
    "    \n",
    "    # ------------------- Add 'trigger' Column -------------------\n",
    "    # For each row, pick the emotion from transformer_emotions with the highest probability.\n",
    "    df_sample['trigger'] = df_sample['transformer_emotions'].apply(\n",
    "        lambda x: max(x, key=x.get) if isinstance(x, dict) and len(x) > 0 else None\n",
    "    )\n",
    "    \n",
    "    # ------------------- Display Sample Results -------------------\n",
    "    print(\"\\n=== Sample Data with Emotion Predictions and Trigger Word ===\")\n",
    "    sample_display = df_sample[['Message', 'nrc_emotions', 'transformer_emotions', 'trigger']].head(5)\n",
    "    for idx, row in sample_display.iterrows():\n",
    "        print(\"-\" * 80)\n",
    "        print(\"Message:\")\n",
    "        print(textwrap.fill(row['Message'], width=80))\n",
    "        print(\"\\nNRCLex Top Emotions:\")\n",
    "        print(row['nrc_emotions'])\n",
    "        print(\"\\nTransformer-Based Emotions:\")\n",
    "        print(row['transformer_emotions'])\n",
    "        print(\"\\nTrigger Emotion (Highest Probability):\")\n",
    "        print(row['trigger'])\n",
    "        print(\"-\" * 80 + \"\\n\")\n",
    "    \n",
    "    # ------------------- Save the Updated DataFrame -------------------\n",
    "    csv_output = \"Aggressive_All_with_trigger.csv\"\n",
    "    json_output = \"Aggressive_All_with_trigger.json\"\n",
    "    \n",
    "    df_sample.to_csv(csv_output, index=False)\n",
    "    df_sample.to_json(json_output, orient='records', lines=True)\n",
    "    \n",
    "    print(\"CSV file saved as:\", csv_output)\n",
    "    print(\"JSON file saved as:\", json_output)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case nltk verifiactions are failed u can use this code to bypass it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMPORARY: bypass SSL error (if it still exists)\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "# TEMPORARY: bypass SSL error (if it still exists)\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color: #FF5733; font-size: 19px;\"> Self assesment tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Python script implements an interactive survey aimed at assessing responses to potentially cyberbullying online messages. It leverages advanced natural language processing by integrating a transformer-based emotion classifier and the NRCLex library. Specifically, the script uses the “bhadresh-savani/distilbert-base-uncased-emotion” model from HuggingFace, chosen for its efficiency and accuracy in identifying a range of emotions in text. The transformer model processes each user-provided message to yield a detailed probability distribution across different emotions, while NRCLex offers an alternative view by extracting the top emotions present. Following the automated analysis, the survey asks users to share how the message makes them feel, rate the intensity of their emotions, comment on the overall mood impact, and describe their potential real-life responses. Each response, along with the computed emotion metrics, is stored and eventually saved as a CSV file. This combined approach not only enriches the survey by providing data-driven insights into emotional triggers associated with cyberbullying but also gathers user perceptions that can be further analyzed to develop effective prevention strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color: #FF5733; font-size: 19px;\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################SURVEY AND MODEL \n",
    "import nltk\n",
    "from transformers import pipeline\n",
    "from nrclex import NRCLex\n",
    "import textwrap\n",
    "import csv\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Initialize the transformer-based emotion classifier (using CPU here)\n",
    "emotion_classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"bhadresh-savani/distilbert-base-uncased-emotion\",\n",
    "    framework=\"pt\",\n",
    "    device=-1,\n",
    "    top_k=None,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "def get_transformer_emotions(text):\n",
    "    \"\"\"Returns a dictionary mapping emotion labels to probabilities using the transformer model.\"\"\"\n",
    "    results = emotion_classifier(text)\n",
    "    return {item['label']: item['score'] for item in results[0]}\n",
    "\n",
    "def get_nrc_emotions(text):\n",
    "    \"\"\"Uses NRCLex to analyze the text and returns top emotions as a list of (emotion, score) tuples.\"\"\"\n",
    "    emotion_obj = NRCLex(text)\n",
    "    return emotion_obj.top_emotions\n",
    "\n",
    "def interactive_survey():\n",
    "    \"\"\"Collects survey responses interactively from the user and returns a list of response records.\"\"\"\n",
    "    print(\"Welcome to the Cyber Bullying Self-Assessment Survey!\")\n",
    "    print(\"You will be asked to input examples of online messages and answer follow-up questions about how they make you feel.\")\n",
    "    print(\"Type 'quit' at any prompt to exit.\\n\")\n",
    "    \n",
    "    responses = []\n",
    "    \n",
    "    while True:\n",
    "        message = input(\"Enter an online message: \").strip()\n",
    "        if message.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        # Get emotion analyses\n",
    "        transformer_emotions = get_transformer_emotions(message)\n",
    "        nrc_emotions = get_nrc_emotions(message)\n",
    "        trigger_emotion = max(transformer_emotions, key=transformer_emotions.get)\n",
    "        \n",
    "        # Ask follow-up questions\n",
    "        user_feeling = input(\"1. How does this message make you feel? \").strip()\n",
    "        if user_feeling.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        intensity = input(\"2. On a scale of 1 (low) to 10 (high), how intense is that feeling? \").strip()\n",
    "        if intensity.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        mood_impact = input(\"3. Does this message affect your mood? (yes/no): \").strip()\n",
    "        if mood_impact.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        response_action = input(\"4. What would you do if you encountered this message in real life? \").strip()\n",
    "        if response_action.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        # Store the response in a dictionary record\n",
    "        record = {\n",
    "            \"message\": message,\n",
    "            \"transformer_emotions\": transformer_emotions,\n",
    "            \"nrc_emotions\": nrc_emotions,\n",
    "            \"trigger\": trigger_emotion,\n",
    "            \"user_feeling\": user_feeling,\n",
    "            \"intensity\": intensity,\n",
    "            \"mood_impact\": mood_impact,\n",
    "            \"response_action\": response_action\n",
    "        }\n",
    "        responses.append(record)\n",
    "        print(\"\\nResponse recorded!\\n\")\n",
    "        cont = input(\"Would you like to assess another message? (yes/no): \").strip().lower()\n",
    "        if cont != \"yes\":\n",
    "            break\n",
    "    \n",
    "    return responses\n",
    "\n",
    "def save_responses_to_csv(responses, filename=\"survey_responses.csv\"):\n",
    "    \"\"\"Saves a list of response records to a CSV file.\"\"\"\n",
    "    if not responses:\n",
    "        print(\"No responses to save.\")\n",
    "        return\n",
    "    # Get the header from keys of the first record.\n",
    "    header = responses[0].keys()\n",
    "    with open(filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=header)\n",
    "        writer.writeheader()\n",
    "        for record in responses:\n",
    "            writer.writerow(record)\n",
    "    print(f\"Responses saved to {filename}\")\n",
    "\n",
    "# Run the interactive survey and save responses to CSV.\n",
    "if __name__ == \"__main__\":\n",
    "    survey_responses = interactive_survey()\n",
    "    save_responses_to_csv(survey_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is designed to analyze survey responses related to cyber bullying by visualizing trends and identifying risk clusters based on user responses. First, it loads survey data from a CSV file and preprocesses the data by converting the ‘intensity’ column to numeric values and transforming the ‘mood_impact’ responses from “yes/no” to a numeric binary format (1 for yes, 0 for no). It then performs a trend analysis by creating bar charts that show the frequency of various trigger emotions (as reported by users) and the average intensity of these emotions.\n",
    "\n",
    "Next, the code builds a feature matrix using the ‘intensity’ and ‘mood_impact_numeric’ columns to assess risk levels among users. It standardizes these features using StandardScaler and applies the KMeans clustering algorithm (with 3 clusters, in this case) to group users who share similar emotional responses. The resulting clusters are then visualized in a scatter plot, where each point represents a user and is colored by its cluster assignment.\n",
    "\n",
    "Overall, this analysis provides data-driven insights by revealing which trigger emotions are most common, how intensely they affect users, and by identifying groups of users who might be at higher risk of adverse emotional impacts from cyber bullying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################## SURVEY PLOT\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the survey responses CSV file\n",
    "df = pd.read_csv(\"survey_responses.csv\")\n",
    "\n",
    "# Inspect the columns\n",
    "print(\"Columns in survey data:\", df.columns.tolist())\n",
    "\n",
    "# For trend analysis, we'll use:\n",
    "# - 'trigger' (categorical trigger emotion)\n",
    "# - 'intensity' (convert to numeric)\n",
    "# - 'mood_impact' (convert yes/no to 1/0)\n",
    "df['intensity'] = pd.to_numeric(df['intensity'], errors='coerce')\n",
    "df['mood_impact_numeric'] = df['mood_impact'].apply(lambda x: 1 if x.strip().lower() == 'yes' else 0)\n",
    "\n",
    "# 1. Trend Analysis\n",
    "# a) Frequency of trigger emotions\n",
    "trigger_counts = df['trigger'].value_counts()\n",
    "print(\"\\nTrigger Emotion Frequencies:\")\n",
    "print(trigger_counts)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "trigger_counts.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "plt.title(\"Frequency of Trigger Emotions\")\n",
    "plt.xlabel(\"Trigger Emotion\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# b) Average intensity by trigger emotion\n",
    "avg_intensity = df.groupby('trigger')['intensity'].mean()\n",
    "print(\"\\nAverage Intensity by Trigger Emotion:\")\n",
    "print(avg_intensity)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "avg_intensity.plot(kind='bar', color='coral', edgecolor='black')\n",
    "plt.title(\"Average Intensity by Trigger Emotion\")\n",
    "plt.xlabel(\"Trigger Emotion\")\n",
    "plt.ylabel(\"Average Intensity (1-10)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Risk Identification via Clustering\n",
    "# We create a feature matrix using 'intensity' and 'mood_impact_numeric'\n",
    "features = df[['intensity', 'mood_impact_numeric']].dropna()\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Determine number of clusters (e.g., 3 clusters)\n",
    "k = 3\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "clusters = kmeans.fit_predict(features_scaled)\n",
    "\n",
    "# Add cluster assignments to the DataFrame\n",
    "df.loc[features.index, 'cluster'] = clusters\n",
    "\n",
    "print(\"\\nCluster Counts:\")\n",
    "print(df['cluster'].value_counts())\n",
    "\n",
    "# Visualize clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(features_scaled[:, 0], features_scaled[:, 1], c=clusters, cmap='viridis', alpha=0.6)\n",
    "plt.title(\"Clustering of Users by Intensity and Mood Impact\")\n",
    "plt.xlabel(\"Standardized Intensity\")\n",
    "plt.ylabel(\"Standardized Mood Impact\")\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Output insights\n",
    "print(\"\\nData-Driven Insights:\")\n",
    "print(\"Trigger Emotion Frequencies:\\n\", trigger_counts)\n",
    "print(\"Average Intensity by Trigger Emotion:\\n\", avg_intensity)\n",
    "print(\"Cluster assignments:\\n\", df['cluster'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
