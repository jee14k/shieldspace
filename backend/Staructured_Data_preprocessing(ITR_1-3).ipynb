{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color: #FF5733; font-size: 56px;\"> Cyber Bullying data preprocessing and wrangling</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script uses the os and pandas libraries to manage and process multiple CSV files stored in the same directory. It iterates over a list of CSV filenames, reads each file into a pandas DataFrame, and then prints essential details such as column names, data types, the shape of the DataFrame, and a preview of the first few rows. If a file can’t be read due to an error, the code prints an error message and continues with the next file, ensuring a clear separation between the outputs of each file. This setup provides a quick and efficient way to explore and understand the structure and content of your CSV data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# List of CSV file names present in the folder.\n",
    "csv_file_list = [\n",
    "    \"aggression_parsed_dataset.csv\",\n",
    "    \"attack_parsed_dataset.csv\",\n",
    "    \"Cyberbullying_Dataset_Summary_Table__Detailed_.csv\",\n",
    "    \"kaggle_parsed_dataset.csv\",\n",
    "    \"twitter_sexism_parsed_dataset.csv\",\n",
    "    \"twitter_racism_parsed_dataset.csv\",\n",
    "    \"twitter_parsed_dataset.csv\",\n",
    "    \"toxicity_parsed_dataset.csv\",\n",
    "    \"youtube_parsed_dataset.csv\",\n",
    "    \"Aggressive_All.csv\",\n",
    "    \"Non_Aggressive_All.csv\"\n",
    "\n",
    "]\n",
    "\n",
    "# Loop through each CSV file in the list.\n",
    "for file_name in csv_file_list:\n",
    "    # Since the CSV files are in the same folder as the notebook,\n",
    "    # the file path is simply the file name.\n",
    "    file_path = os.path.join(file_name)\n",
    "    \n",
    "    # Print header for current file processing.\n",
    "    print(f\"=== Processing File: {file_name} ===\")\n",
    "    \n",
    "    # Try to read the CSV file into a pandas DataFrame.\n",
    "    try:\n",
    "        data_frame = pd.read_csv(file_path)\n",
    "    except Exception as error:\n",
    "        print(f\"Error reading {file_name}: {error}\")\n",
    "        print(\"-\" * 60 + \"\\n\")\n",
    "        continue  # Skip to the next file if an error occurs.\n",
    "    \n",
    "    # Display the column names of the DataFrame.\n",
    "    print(\"Column Names:\")\n",
    "    print(data_frame.columns.tolist())\n",
    "    \n",
    "    # Display the data types of each column.\n",
    "    print(\"\\nData Types:\")\n",
    "    print(data_frame.dtypes)\n",
    "    \n",
    "    # Display the number of rows and columns in the DataFrame.\n",
    "    num_rows, num_columns = data_frame.shape\n",
    "    print(f\"\\nNumber of Rows: {num_rows}\")\n",
    "    print(f\"Number of Columns: {num_columns}\")\n",
    "    \n",
    "    # Display the first 5 rows of the DataFrame as a preview.\n",
    "    print(\"\\nFirst 5 Rows:\")\n",
    "    print(data_frame.head())\n",
    "    \n",
    "    # Print a separator line after processing each file.\n",
    "    print(\"\\n\" + \"-\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color: green; font-size: 19px;\"> Data wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script begins by loading the CSV file into a DataFrame, which serves as the initial dataset. It then checks for duplicate rows by counting and printing their number, and subsequently removes any duplicates found. In addition, the script identifies rows that are entirely empty—meaning all values are missing—counts and prints these empty rows, and removes them from the DataFrame. Finally, it saves the cleaned DataFrame to a new CSV file, ensuring that all modifications are preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the file name (update with our file name)\n",
    "file_name = 'your_file.csv' #put the file we want to check \n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "# Check for duplicate rows\n",
    "num_duplicates = df.duplicated().sum()\n",
    "print(f\"Found {num_duplicates} duplicate rows.\")\n",
    "\n",
    "# Remove duplicate rows, if any\n",
    "if num_duplicates > 0:\n",
    "    df = df.drop_duplicates()\n",
    "    print(\"Duplicate rows have been removed.\")\n",
    "\n",
    "# Check for empty rows (rows where all cells are NaN)\n",
    "num_empty_rows = df.isnull().all(axis=1).sum()\n",
    "print(f\"Found {num_empty_rows} empty rows.\")\n",
    "\n",
    "# Remove empty rows, if any\n",
    "if num_empty_rows > 0:\n",
    "    df = df.dropna(how='all')\n",
    "    print(\"Empty rows have been removed.\")\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "clean_file_name = 'your_file_cleaned.csv'# \n",
    "df.to_csv(clean_file_name, index=False)\n",
    "print(f\"Cleaned data has been saved to {clean_file_name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Python script performs text cleaning and preprocessing on selected CSV datasets. It removes null or empty rows, standardizes the text by lowercasing and stripping extra spaces, and then saves cleaned versions of each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --------------------- Helper Function ---------------------\n",
    "def clean_text(text):\n",
    "    \"\"\"Cleans input text by lowercasing it and removing extra whitespace.\"\"\"\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    text = str(text).lower()          # Convert to lowercase\n",
    "    text = \" \".join(text.split())     # Remove extra spaces/newlines\n",
    "    return text\n",
    "\n",
    "# --------------------- File & Column Mapping ---------------------\n",
    "# Dictionary mapping each file name to the list of text columns to clean.\n",
    "file_info = {\n",
    "    \"aggression_parsed_dataset.csv\": [\"Text\"],\n",
    "    \"attack_parsed_dataset.csv\": [\"Text\"],\n",
    "    \"toxicity_parsed_dataset.csv\": [\"Text\"],\n",
    "    \"Aggressive_All.csv\": [\"Message\"],\n",
    "    \"Non_Aggressive_All.csv\": [\"Message\"]\n",
    "}\n",
    "\n",
    "# --------------------- Processing Each File ---------------------\n",
    "for file_name, text_columns in file_info.items():\n",
    "    if os.path.exists(file_name):\n",
    "        print(f\"Processing file: {file_name}\")\n",
    "        df = pd.read_csv(file_name)\n",
    "        \n",
    "        # Process each text column specified for this file\n",
    "        for col in text_columns:\n",
    "            if col in df.columns:\n",
    "                # Remove rows where the column is missing or empty\n",
    "                df = df[~(df[col].isnull() | (df[col].astype(str).str.strip() == \"\"))]\n",
    "                # Clean the column\n",
    "                df[col] = df[col].apply(clean_text)\n",
    "            else:\n",
    "                print(f\"Warning: Column '{col}' not found in {file_name}\")\n",
    "\n",
    "        # Save the cleaned DataFrame to a new CSV file\n",
    "        new_file_name = file_name.replace(\".csv\", \"\") + \"_cleaned.csv\"\n",
    "        df.to_csv(new_file_name, index=False, encoding=\"utf-8\")\n",
    "        print(f\"Saved cleaned file: {new_file_name}\\n\")\n",
    "    else:\n",
    "        print(f\"File {file_name} does not exist.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script performs statistical analysis and visualization on three cyberbullying-related datasets: aggression_parsed_dataset.csv, attack_parsed_dataset.csv, and toxicity_parsed_dataset.csv. It focuses on the ed_label_0 column, converting it to numeric form, removing missing values, and printing descriptive statistics for each dataset. A one-way ANOVA test is then conducted to determine if there are statistically significant differences in ed_label_0 values across the three datasets. The results are visualized using a boxplot. Additionally, the script analyzes the youtube_parsed_dataset.csv by examining the distribution of user ages and their correlation with the number of comments. It calculates Pearson’s correlation coefficient and visualizes the relationship using a scatter plot. Overall, this analysis provides insights into the intensity of aggression across sources and user behavior trends on YouTube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --------------------- Aggression, Attack, and Toxicity Datasets ---------------------\n",
    "# Load the CSV files\n",
    "df_aggression = pd.read_csv(\"aggression_parsed_dataset.csv\")\n",
    "df_attack = pd.read_csv(\"attack_parsed_dataset.csv\")\n",
    "df_toxicity = pd.read_csv(\"toxicity_parsed_dataset.csv\")\n",
    "\n",
    "# For these datasets, we use the \"ed_label_0\" column (ensure it is numeric)\n",
    "df_aggression[\"ed_label_0\"] = pd.to_numeric(df_aggression[\"ed_label_0\"], errors='coerce')\n",
    "df_attack[\"ed_label_0\"]     = pd.to_numeric(df_attack[\"ed_label_0\"], errors='coerce')\n",
    "df_toxicity[\"ed_label_0\"]   = pd.to_numeric(df_toxicity[\"ed_label_0\"], errors='coerce')\n",
    "\n",
    "# Remove any missing values from ed_label_0\n",
    "group_aggression = df_aggression[\"ed_label_0\"].dropna()\n",
    "group_attack     = df_attack[\"ed_label_0\"].dropna()\n",
    "group_toxicity   = df_toxicity[\"ed_label_0\"].dropna()\n",
    "\n",
    "# Print descriptive statistics\n",
    "print(\"Descriptive Statistics for 'ed_label_0':\\n\")\n",
    "print(\"Aggression Dataset:\")\n",
    "print(group_aggression.describe())\n",
    "print(\"\\nAttack Dataset:\")\n",
    "print(group_attack.describe())\n",
    "print(\"\\nToxicity Dataset:\")\n",
    "print(group_toxicity.describe())\n",
    "\n",
    "# --------------------- One-Way ANOVA Test ---------------------\n",
    "f_stat, p_val = stats.f_oneway(group_aggression, group_attack, group_toxicity)\n",
    "print(\"\\nOne-Way ANOVA Results on 'ed_label_0':\")\n",
    "print(f\"  F-statistic = {f_stat:.3f}\")\n",
    "print(f\"  p-value = {p_val:.3f}\")\n",
    "\n",
    "# --------------------- Boxplot Visualization ---------------------\n",
    "# Combine the three groups in a single DataFrame for plotting\n",
    "data = pd.concat([group_aggression, group_attack, group_toxicity], axis=0)\n",
    "group_labels = ([\"Aggression\"] * len(group_aggression)) + \\\n",
    "               ([\"Attack\"] * len(group_attack)) + \\\n",
    "               ([\"Toxicity\"] * len(group_toxicity))\n",
    "df_plot = pd.DataFrame({\"ed_label_0\": data, \"Dataset\": group_labels})\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x=\"Dataset\", y=\"ed_label_0\", data=df_plot, palette=\"Set2\")\n",
    "plt.title(\"Distribution of 'ed_label_0' Across Datasets\")\n",
    "plt.xlabel(\"Dataset\")\n",
    "plt.ylabel(\"ed_label_0\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# --------------------- YouTube Parsed Dataset ---------------------\n",
    "df_youtube = pd.read_csv(\"youtube_parsed_dataset.csv\")\n",
    "print(\"\\n=== YouTube Parsed Dataset ===\")\n",
    "print(\"Column Names:\", df_youtube.columns.tolist())\n",
    "\n",
    "# For example, examine descriptive statistics for 'Age'\n",
    "df_youtube[\"Age\"] = pd.to_numeric(df_youtube[\"Age\"], errors='coerce')\n",
    "print(\"Descriptive Statistics for 'Age':\")\n",
    "print(df_youtube[\"Age\"].describe())\n",
    "\n",
    "# And examine correlation between 'Age' and 'Number of Comments'\n",
    "df_youtube[\"Number of Comments\"] = pd.to_numeric(df_youtube[\"Number of Comments\"], errors='coerce')\n",
    "corr, p_val_corr = stats.pearsonr(df_youtube[\"Age\"].dropna(), df_youtube[\"Number of Comments\"].dropna())\n",
    "print(\"\\nPearson Correlation between Age and Number of Comments:\")\n",
    "print(f\"  Correlation = {corr:.3f}\")\n",
    "print(f\"  p-value = {p_val_corr:.3f}\")\n",
    "\n",
    "# Optional: Scatter plot of Age vs. Number of Comments\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=\"Age\", y=\"Number of Comments\", data=df_youtube, alpha=0.7)\n",
    "plt.title(\"Scatter Plot: Age vs. Number of Comments\")\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Number of Comments\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "from scipy.stats import chisquare\n",
    "\n",
    "# Load the CB Multi-Labeled Balanced Dataset\n",
    "df_cb = pd.read_csv(\"cb_multi_labeled_balanced.csv\")\n",
    "\n",
    "print(\"=== CB Multi-Labeled Balanced Dataset ===\")\n",
    "print(\"Column Names:\", df_cb.columns.tolist())\n",
    "print(\"Descriptive Statistics (Label counts):\")\n",
    "print(df_cb[\"label\"].value_counts())\n",
    "\n",
    "# Create the contingency counts: observed frequencies for each label.\n",
    "observed = df_cb[\"label\"].value_counts().sort_index()\n",
    "# For a goodness-of-fit test, we assume a uniform distribution:\n",
    "expected = [observed.sum() / len(observed)] * len(observed)\n",
    "\n",
    "# Perform chi-square goodness-of-fit test\n",
    "chi2, p_val = chisquare(f_obs=observed, f_exp=expected)\n",
    "print(\"\\nChi-Square Goodness-of-Fit Test for 'label':\")\n",
    "print(f\"  Chi-square Statistic = {chi2:.3f}\")\n",
    "print(f\"  p-value = {p_val:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 style=\"color: #FF5733; font-size: 19px;\">Natural language processing  and analysis in file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview of the Process\n",
    "\n",
    "In our pipeline, we began by loading and preprocessing a cleaned CSV file containing messages. We removed any rows with empty messages and standardized the text by converting it to lowercase and stripping extra whitespace. This ensured that our subsequent analysis was performed on a consistent and high-quality dataset.\n",
    "\n",
    "Emotion Analysis Approaches\n",
    "\n",
    "We employed two distinct emotion analysis methods. First, we used a lexicon-based approach with NRCLex. NRCLex utilizes the NRC Emotion Lexicon, which maps words to basic emotions such as anger, fear, joy, sadness, and disgust. By analyzing the frequency of these emotion-related words in a message, NRCLex returns a list of top emotions with their associated scores. This method is fast and interpretable, providing a straightforward snapshot of the emotional cues in the text.\n",
    "\n",
    "In addition, we applied a transformer-based approach using a pretrained model, specifically “bhadresh-savani/distilbert-base-uncased-emotion”. This model is a distilled version of BERT that has been fine-tuned for emotion classification. It predicts multiple emotion categories—such as anger, joy, sadness, fear, love, and surprise—by returning a probability distribution over these labels for each message. We chose this model because DistilBERT is lighter and faster than the full BERT model while maintaining strong performance, and because it offers a detailed, probability-based classification that can capture nuanced emotional content.\n",
    "\n",
    "Creating the Trigger Column\n",
    "\n",
    "After obtaining the transformer-based emotion predictions, we created an additional column labeled “trigger.” This column identifies the dominant emotion for each message by selecting the emotion with the highest probability from the transformer’s output. This trigger word serves as an immediate indicator of the primary emotional signal in a message, which can be very useful for further analysis or for developing interactive tools such as quizzes aimed at cyber bullying prevention.\n",
    "\n",
    "Saving the Results\n",
    "\n",
    "Finally, the enriched DataFrame—now containing the original messages, NRCLex emotion outputs, transformer-based emotion probabilities, and the trigger column—is saved in both CSV and JSON formats. The CSV file offers ease of use for further data manipulation or viewing in spreadsheet applications, while the JSON format is ideal for integration with web-based applications and interactive educational tools.\n",
    "\n",
    "By combining these methods, we achieve a comprehensive understanding of the emotional content in each message, enabling more informed analysis and effective strategies for cyber bullying prevention and educational initiatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nrclex import NRCLex\n",
    "from transformers import pipeline\n",
    "import textwrap\n",
    "import torch\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "import logging\n",
    "\n",
    "# Set logging level for transformers to suppress informational messages\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "# Verify PyTorch installation\n",
    "print(\"Installed PyTorch version:\", torch.__version__)\n",
    "print(\"Number of GPUs available:\", torch.cuda.device_count())\n",
    "\n",
    "# Download necessary NLTK data (if needed)\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# ------------------- Load and Preprocess Data -------------------\n",
    "file_name = \"Aggressive_All_cleaned.csv\"\n",
    "df = pd.read_csv(file_name)\n",
    "print(\"Columns in dataset:\", df.columns.tolist())\n",
    "\n",
    "# Remove rows where 'Message' is missing or empty\n",
    "df = df[~(df['Message'].isnull() | (df['Message'].astype(str).str.strip() == \"\"))]\n",
    "print(\"Number of rows after cleaning empty messages:\", len(df))\n",
    "\n",
    "# Standardize messages: convert to lowercase and strip extra whitespace\n",
    "df['Message'] = df['Message'].astype(str).str.lower().str.strip()\n",
    "\n",
    "# ------------------- Define Emotion Analysis Functions -------------------\n",
    "\n",
    "def get_nrc_emotions(text):\n",
    "    \"\"\"\n",
    "    Uses NRCLex to analyze the text and returns the top emotions as a list of (emotion, score) tuples.\n",
    "    \"\"\"\n",
    "    emotion_obj = NRCLex(text)\n",
    "    return emotion_obj.top_emotions\n",
    "\n",
    "def convert_transformer_results(results):\n",
    "    \"\"\"Convert a list of dictionaries to a single dictionary mapping emotion labels to scores.\"\"\"\n",
    "    return {item['label']: item['score'] for item in results}\n",
    "\n",
    "def process_batch(batch):\n",
    "    \"\"\"\n",
    "    Processes a batch of messages using the transformer pipeline.\n",
    "    Each process initializes its own pipeline instance.\n",
    "    \"\"\"\n",
    "    # Initialize the pipeline inside each process using CPU (change device=0 if using GPU)\n",
    "    emotion_classifier = pipeline(\n",
    "        \"text-classification\", \n",
    "        model=\"bhadresh-savani/distilbert-base-uncased-emotion\", \n",
    "        framework=\"pt\",        # explicitly use PyTorch\n",
    "        device=-1,             # use CPU; change to device=0 if GPU is available\n",
    "        top_k=None,            # equivalent to return_all_scores=True\n",
    "        truncation=True        # truncate texts longer than model's max length\n",
    "    )\n",
    "    results = emotion_classifier(batch)\n",
    "    # Convert each result (a list of dictionaries) into a single dictionary\n",
    "    processed = [convert_transformer_results(result) for result in results]\n",
    "    return processed\n",
    "\n",
    "# ------------------- Parallel Processing for Transformer Emotions -------------------\n",
    "def main():\n",
    "    # For faster experimentation, you can sample a subset (remove or adjust sampling as needed)\n",
    "    sample_size = 5000  # change or remove to process full dataset\n",
    "    df_sample = df.sample(n=sample_size, random_state=42)\n",
    "    messages = df_sample['Message'].tolist()\n",
    "    \n",
    "    batch_size = 32  # adjust based on our hardware\n",
    "    batches = [messages[i:i+batch_size] for i in range(0, len(messages), batch_size)]\n",
    "    \n",
    "    print(f\"Total messages in sample: {len(messages)}\")\n",
    "    print(f\"Total batches (batch size={batch_size}): {len(batches)}\")\n",
    "    \n",
    "    # Use multiprocessing Pool to process batches in parallel\n",
    "    with mp.Pool(mp.cpu_count()) as pool:\n",
    "        transformer_results = pool.map(process_batch, batches)\n",
    "    \n",
    "    # Flatten the list of lists into a single list for each message's transformer predictions\n",
    "    transformer_emotions = [item for sublist in transformer_results for item in sublist]\n",
    "    df_sample['transformer_emotions'] = transformer_emotions\n",
    "    \n",
    "    # Process NRCLex emotions (this step remains sequential)\n",
    "    print(\"Processing NRCLex emotions...\")\n",
    "    df_sample['nrc_emotions'] = df_sample['Message'].apply(get_nrc_emotions)\n",
    "    \n",
    "    # ------------------- Add 'trigger' Column -------------------\n",
    "    # For each row, pick the emotion from transformer_emotions with the highest probability.\n",
    "    df_sample['trigger'] = df_sample['transformer_emotions'].apply(\n",
    "        lambda x: max(x, key=x.get) if isinstance(x, dict) and len(x) > 0 else None\n",
    "    )\n",
    "    \n",
    "    # ------------------- Display Sample Results -------------------\n",
    "    print(\"\\n=== Sample Data with Emotion Predictions and Trigger Word ===\")\n",
    "    sample_display = df_sample[['Message', 'nrc_emotions', 'transformer_emotions', 'trigger']].head(5)\n",
    "    for idx, row in sample_display.iterrows():\n",
    "        print(\"-\" * 80)\n",
    "        print(\"Message:\")\n",
    "        print(textwrap.fill(row['Message'], width=80))\n",
    "        print(\"\\nNRCLex Top Emotions:\")\n",
    "        print(row['nrc_emotions'])\n",
    "        print(\"\\nTransformer-Based Emotions:\")\n",
    "        print(row['transformer_emotions'])\n",
    "        print(\"\\nTrigger Emotion (Highest Probability):\")\n",
    "        print(row['trigger'])\n",
    "        print(\"-\" * 80 + \"\\n\")\n",
    "    \n",
    "    # ------------------- Save the Updated DataFrame -------------------\n",
    "    csv_output = \"Aggressive_All_with_trigger.csv\"\n",
    "    json_output = \"Aggressive_All_with_trigger.json\"\n",
    "    \n",
    "    df_sample.to_csv(csv_output, index=False)\n",
    "    df_sample.to_json(json_output, orient='records', lines=True)\n",
    "    \n",
    "    print(\"CSV file saved as:\", csv_output)\n",
    "    print(\"JSON file saved as:\", json_output)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case nltk verifiactions are failed u can use this code to bypass it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMPORARY: bypass SSL error (if it still exists)\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "# TEMPORARY: bypass SSL error (if it still exists)\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color: #FF5733; font-size: 19px;\">Youtube data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation\n",
    "\t1.\tVADER Sentiment Analysis:\n",
    "The script uses NLTK’s VADER (Valence Aware Dictionary and sEntiment Reasoner) to calculate sentiment scores for each comment. The classify_sentiment_vader function computes the compound score—a single metric that summarizes the overall sentiment. If the compound score is below a specified threshold (here, -0.5), the text is flagged as “Aggressive”; otherwise, it is marked as “Non-Aggressive.” Additionally, the complete set of sentiment scores is returned for further analysis.\n",
    "\t2.\tData Loading and Processing:\n",
    "The CSV file is loaded into a pandas DataFrame. Sentiment analysis is applied to the “Text” column, and two new columns are added: one for the sentiment classification and one for the detailed emotion scores.\n",
    "\t3.\tText Wrapping for Better Output:\n",
    "A helper function wraps the text in the “Text” column so that when the DataFrame is printed, long comments are neatly formatted over multiple lines.\n",
    "\t4.\tSaving the Results:\n",
    "The updated DataFrame is saved to a new CSV file (“youtube_parsed_dataset_sentiment.csv”), preserving all changes for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import textwrap\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download the VADER lexicon (if not already downloaded)\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Initialize VADER sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def classify_sentiment_vader(text, threshold=-0.5):\n",
    "    \"\"\"\n",
    "    Classify sentiment using VADER. \n",
    "    If the compound score is below the threshold (e.g., -0.5), flag as Aggressive; otherwise, Non-Aggressive.\n",
    "    Returns the sentiment label along with the complete score dictionary.\n",
    "    \"\"\"\n",
    "    scores = sia.polarity_scores(text)\n",
    "    sentiment = \"Aggressive\" if scores['compound'] < threshold else \"Non-Aggressive\"\n",
    "    return sentiment, scores\n",
    "\n",
    "# Load the CSV file containing the YouTube dataset.\n",
    "df = pd.read_csv(\"youtube_parsed_dataset.csv\")\n",
    "\n",
    "# Apply VADER sentiment analysis on the \"Text\" column.\n",
    "df[\"sentiment_class\"], df[\"emotion_scores\"] = zip(*df[\"Text\"].apply(classify_sentiment_vader))\n",
    "\n",
    "# Function to wrap long text for display\n",
    "def wrap_text(text, width=80):\n",
    "    return \"\\n\".join(textwrap.wrap(text, width=width))\n",
    "\n",
    "# Create a copy of the DataFrame for printing with wrapped \"Text\" column.\n",
    "df_wrapped = df.copy()\n",
    "df_wrapped[\"Text\"] = df_wrapped[\"Text\"].apply(lambda x: wrap_text(x, width=80))\n",
    "\n",
    "# Print the DataFrame with wrapped text.\n",
    "print(\"Updated DataFrame with Sentiment Analysis:\\n\")\n",
    "print(df_wrapped.to_string())\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file.\n",
    "df.to_csv(\"youtube_parsed_dataset_sentiment.csv\", index=False)\n",
    "print(\"\\nSentiment analysis complete. Results saved to 'youtube_parsed_dataset_sentiment.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\textcolor{red}{\\textbf{\\small \\text{model training}}}\n",
    "$$\n",
    "This script fine-tunes the Toxic-BERT model (unitary/toxic-bert) for binary text classification using two datasets: attack_parsed_dataset.csv and toxicity_parsed_dataset.csv. It begins by combining the datasets, ensuring the oh_label column is binary (0 or 1), and cleaning the text. The cleaned data is then converted into a Hugging Face DatasetDict and split into training and validation sets. Using the tokenizer from the pre-trained Toxic-BERT checkpoint, each text is tokenized with padding and truncation. The model is then fine-tuned for 3 epochs with evaluation at each epoch using accuracy as the metric. Finally, the model is evaluated and saved locally under the same model name. This pipeline enables building a custom toxicity detector adapted to our dataset for real-world message classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# --------------------- Load & Prepare Data ---------------------\n",
    "# Load our toxicity datasets\n",
    "df_attack = pd.read_csv(\"attack_parsed_dataset.csv\")\n",
    "df_toxicity = pd.read_csv(\"toxicity_parsed_dataset.csv\")\n",
    "\n",
    "# Ensure that the label is integer (0 or 1)\n",
    "df_attack[\"oh_label\"] = df_attack[\"oh_label\"].astype(int)\n",
    "df_toxicity[\"oh_label\"] = df_toxicity[\"oh_label\"].astype(int)\n",
    "\n",
    "# Combine the datasets\n",
    "df_combined = pd.concat([df_attack, df_toxicity], ignore_index=True)\n",
    "\n",
    "# Remove rows where \"Text\" is missing or empty, and clean the text.\n",
    "df_combined = df_combined[df_combined[\"Text\"].notnull()].copy()\n",
    "df_combined[\"Text\"] = df_combined[\"Text\"].astype(str).str.lower().str.strip()\n",
    "\n",
    "# Keep only the columns \"Text\" and \"oh_label\"\n",
    "df_combined = df_combined[[\"Text\", \"oh_label\"]]\n",
    "\n",
    "# Convert to Hugging Face Dataset and split\n",
    "dataset = Dataset.from_pandas(df_combined)\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": dataset[\"train\"],\n",
    "    \"validation\": dataset[\"test\"]\n",
    "})\n",
    "\n",
    "# --------------------- Tokenization ---------------------\n",
    "model_checkpoint = \"unitary/toxic-bert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"Text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset_dict.map(tokenize_function, batched=True)\n",
    "\n",
    "# --------------------- Model & Training Setup ---------------------\n",
    "# Load the Toxic-BERT model for sequence classification with 2 labels.\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./toxic_model_results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# --------------------- Train & Evaluate ---------------------\n",
    "trainer.train()\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", eval_results)\n",
    "\n",
    "# Save the fine-tuned model with the same name as the original.\n",
    "# This will create a folder named \"unitary/toxic-bert\" locally.\n",
    "trainer.save_model(\"unitary/toxic-bert\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\textcolor{teal}{\\textbf{\\small \\text{Fine-Tuning RoBERTa Emotion Model on Aggression Datasets}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code fine-tunes the SamLowe/roberta-base-go_emotions model to classify aggressive vs. non-aggressive text using three cleaned datasets. It tokenizes the text, splits the data, and trains the model using Hugging Face’s Trainer API. The final model is evaluated for accuracy and saved for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# --------------------- Load & Prepare Data ---------------------\n",
    "# Load aggressive and non-aggressive datasets\n",
    "df_aggressive = pd.read_csv(\"Aggressive_All.csv\")\n",
    "df_non_aggressive = pd.read_csv(\"Non_Aggressive_All.csv\")\n",
    "df_aggression = pd.read_csv(\"aggression_parsed_dataset.csv\")\n",
    "\n",
    "# Assign binary labels: aggressive = 1, non-aggressive = 0.\n",
    "df_aggressive[\"label\"] = 1\n",
    "df_non_aggressive[\"label\"] = 0\n",
    "# For aggression_parsed_dataset.csv, assume the \"oh_label\" column (convert to integer)\n",
    "df_aggression[\"label\"] = df_aggression[\"oh_label\"].astype(int)\n",
    "\n",
    "# Standardize text columns. Adjust the column names if necessary.\n",
    "df_aggressive[\"text\"] = df_aggressive[\"Message\"].astype(str).str.lower().str.strip()\n",
    "df_non_aggressive[\"text\"] = df_non_aggressive[\"Message\"].astype(str).str.lower().str.strip()\n",
    "df_aggression[\"text\"] = df_aggression[\"Text\"].astype(str).str.lower().str.strip()\n",
    "\n",
    "# Keep only the relevant columns (\"text\" and \"label\")\n",
    "df_aggressive = df_aggressive[[\"text\", \"label\"]]\n",
    "df_non_aggressive = df_non_aggressive[[\"text\", \"label\"]]\n",
    "df_aggression = df_aggression[[\"text\", \"label\"]]\n",
    "\n",
    "# Combine the datasets\n",
    "df_combined = pd.concat([df_aggressive, df_non_aggressive, df_aggression], ignore_index=True)\n",
    "# Remove rows with empty text\n",
    "df_combined = df_combined[df_combined[\"text\"].str.strip() != \"\"]\n",
    "\n",
    "# Convert to Hugging Face Dataset and split\n",
    "dataset = Dataset.from_pandas(df_combined)\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "dataset_dict = DatasetDict({\"train\": dataset[\"train\"], \"validation\": dataset[\"test\"]})\n",
    "\n",
    "# --------------------- Tokenization ---------------------\n",
    "model_checkpoint = \"SamLowe/roberta-base-go_emotions\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset_dict.map(tokenize_function, batched=True)\n",
    "\n",
    "# --------------------- Model & Training Setup ---------------------\n",
    "# Load the emotion model for sequence classification; set num_labels=2 for binary classification.\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./emotion_model_results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# --------------------- Train & Evaluate ---------------------\n",
    "trainer.train()\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", eval_results)\n",
    "\n",
    "# Save the fine-tuned model using the same name as the original.\n",
    "trainer.save_model(\"SamLowe/roberta-base-go_emotions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\textcolor{orange}{\\textbf{\\small \\text{Iteration 1 – Emotion and Toxicity Detection Model}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This interactive script uses  GoEmotions and Toxic-BERT trained from our dataset  to analyze the emotional tone and toxicity of user-inputted text. It identifies the top 5 emotions, flags risk levels, and highlights categories like insults or threats. The model runs in a loop, providing real-time insights for every message entered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "from transformers import pipeline\n",
    "import textwrap\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# --------------------- Initialize Models ---------------------\n",
    "# Emotion analysis model using SamLowe's GoEmotions model for improved emotion detection\n",
    "emotion_classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"SamLowe/roberta-base-go_emotions\",\n",
    "    top_k=None,         # Return all scores then we'll pick the top 5\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# Toxicity detection model using Toxic-BERT\n",
    "toxicity_classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"unitary/toxic-bert\",\n",
    "    top_k=None,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# --------------------- Helper Functions ---------------------\n",
    "def get_transformer_emotions(text):\n",
    "    \"\"\"\n",
    "    Uses the GoEmotions model to compute emotion scores.\n",
    "    Returns the top 5 emotions (sorted by score) as a dictionary.\n",
    "    \"\"\"\n",
    "    results = emotion_classifier(text)[0]\n",
    "    # Sort the results in descending order by score, and take the top 5 entries\n",
    "    sorted_results = sorted(results, key=lambda x: x['score'], reverse=True)[:5]\n",
    "    return {item['label']: item['score'] for item in sorted_results}\n",
    "\n",
    "def get_toxicity_score(text):\n",
    "    \"\"\"\n",
    "    Uses the Toxic-BERT model to compute toxicity scores.\n",
    "    Returns a dictionary mapping each toxicity label to its score.\n",
    "    \"\"\"\n",
    "    results = toxicity_classifier(text)[0]\n",
    "    return {item['label']: item['score'] for item in results}\n",
    "\n",
    "def display_analysis(message):\n",
    "    \"\"\"\n",
    "    Performs and displays both emotion analysis (top 5) and toxicity analysis\n",
    "    with friendly labels and detected tags.\n",
    "    \"\"\"\n",
    "    # Emotion Analysis\n",
    "    emotions = get_transformer_emotions(message)\n",
    "    trigger_emotion = max(emotions, key=emotions.get)\n",
    "    \n",
    "    # Toxicity Analysis\n",
    "    toxicity = get_toxicity_score(message)\n",
    "    toxic_level = toxicity.get('toxic', 0.0)\n",
    "    \n",
    "    # Determine friendly toxicity label based on the toxicity score.\n",
    "    if toxic_level > 0.85:\n",
    "        friendly_tox_level = \"🔥 Highly Toxic\"\n",
    "    elif toxic_level > 0.5:\n",
    "        friendly_tox_level = \"⚠️ Possibly Offensive\"\n",
    "    elif toxic_level > 0.2:\n",
    "        friendly_tox_level = \"🟡 Mildly Risky\"\n",
    "    else:\n",
    "        friendly_tox_level = \"✅ Low or Safe\"\n",
    "    \n",
    "    # Generate descriptive tags if certain toxicity sub-scores exceed thresholds.\n",
    "    tags = []\n",
    "    if toxicity.get(\"insult\", 0) > 0.6:\n",
    "        tags.append(\"🔴 Insult\")\n",
    "    if toxicity.get(\"obscene\", 0) > 0.6:\n",
    "        tags.append(\"🤬 Obscene\")\n",
    "    if toxicity.get(\"severe_toxic\", 0) > 0.4:\n",
    "        tags.append(\"🚨 Severe Toxicity\")\n",
    "    if toxicity.get(\"identity_hate\", 0) > 0.4:\n",
    "        tags.append(\"🛑 Identity Hate\")\n",
    "    if toxicity.get(\"threat\", 0) > 0.3:\n",
    "        tags.append(\"⚠️ Threat\")\n",
    "    tags_str = \", \".join(tags) if tags else \"No critical flags\"\n",
    "    \n",
    "    # Print the analysis results.\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"Message Analyzed:\")\n",
    "    print(textwrap.fill(message, width=80))\n",
    "    \n",
    "    print(\"\\n💡 Emotion Analysis (Top 5):\")\n",
    "    for label, score in emotions.items():\n",
    "        print(f\"  {label}: {score:.3f}\")\n",
    "    print(f\"\\n🧠 Primary Emotion: {trigger_emotion}\")\n",
    "    \n",
    "    print(\"\\n⚠️ Toxicity Analysis:\")\n",
    "    print(f\"  Toxicity Level: {friendly_tox_level}\")\n",
    "    print(f\"  Detected Tags: {tags_str}\")\n",
    "    print(\"-\" * 80 + \"\\n\")\n",
    "\n",
    "def main():\n",
    "    print(\"Enter a message to analyze its emotions and toxicity.\")\n",
    "    print(\"Type 'quit' at any time to exit.\\n\")\n",
    "    \n",
    "    while True:\n",
    "        message = input(\"Message: \").strip()\n",
    "        if message.lower() == 'quit':\n",
    "            print(\"Exiting. Thank you!\")\n",
    "            break\n",
    "        display_analysis(message)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\textcolor{purple}{\\textbf{\\small \\text{Full-Stack Message Analyzer – Iteration 2}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script combines three models—GoEmotions, Toxic-BERT, and a fine-tuned cyberbullying detector—to perform full-stack emotional, toxicity, and cyberbullying analysis on user-inputted messages. It uses Hugging Face pipelines and a custom .pth checkpoint to classify both general and targeted online harms. Results include emotion breakdowns, toxicity levels, and specific cyberbullying labels like age or religion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Full-stack message analyser\n",
    "───────────────────────────\n",
    "• Emotions                 (SamLowe/roberta-base-go_emotions)\n",
    "• General toxicity         (unitary/toxic-bert)\n",
    "• Cyber-bullying detector  (jkos0012/bert-cyberbullying – .pth weights)\n",
    "\"\"\"\n",
    "\n",
    "import textwrap, torch\n",
    "from huggingface_hub import hf_hub_download\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoTokenizer,\n",
    "    BertConfig,\n",
    "    BertForSequenceClassification,\n",
    "    TextClassificationPipeline,\n",
    ")\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "# 1.  Optional: NLTK data (uncomment only if you really need it)\n",
    "# import nltk\n",
    "# nltk.download(\"punkt\")\n",
    "# nltk.download(\"vader_lexicon\")\n",
    "# ───────────────────────────────────────────────────────────────\n",
    "\n",
    "# 2.  Emotion & general-toxicity classifiers (unchanged)\n",
    "emotion_classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"SamLowe/roberta-base-go_emotions\",\n",
    "    top_k=None,\n",
    "    truncation=True,\n",
    ")\n",
    "\n",
    "toxicity_classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"unitary/toxic-bert\",\n",
    "    top_k=None,\n",
    "    truncation=True,\n",
    ")\n",
    "\n",
    "# 3.  Cyber-bullying detector built from raw .pth checkpoint\n",
    "REPO_ID     = \"jkos0012/bert-cyberbullying\"   # <— new repo\n",
    "PT_FILE     = \"bert_cyberbullying.pth\"        # file inside repo\n",
    "BASE_MODEL  = \"bert-base-uncased\"             # backbone\n",
    "\n",
    "# 3-A  download & load the checkpoint\n",
    "ckpt_path  = hf_hub_download(REPO_ID, filename=PT_FILE)\n",
    "state_dict = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "\n",
    "# 3-B  infer output-layer size (e.g. 2 × 768)\n",
    "num_labels = state_dict[\"classifier.weight\"].shape[0]\n",
    "\n",
    "# 3-C  give the two output neurons real names\n",
    "LABELS = [\"religion\", \"age\"]          # index 0 → age, index 1 → religion\n",
    "id2label = dict(enumerate(LABELS))\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "# 3-D  build a compatible BERT config & model\n",
    "config = BertConfig.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "cyber_model = BertForSequenceClassification(config)\n",
    "cyber_model.load_state_dict(state_dict, strict=True)   # <— no more size-mismatch!\n",
    "\n",
    "# 3-E  wrap in a normal HF pipeline\n",
    "cyberbullying_classifier = TextClassificationPipeline(\n",
    "    model=cyber_model,\n",
    "    tokenizer=AutoTokenizer.from_pretrained(BASE_MODEL),\n",
    "    function_to_apply=\"sigmoid\",   # multi-label\n",
    "    top_k=None,\n",
    ")\n",
    "\n",
    "# 4.  Helper functions\n",
    "def get_top5_emotions(text: str) -> dict:\n",
    "    res = emotion_classifier(text)[0]\n",
    "    res.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "    return {r[\"label\"]: r[\"score\"] for r in res[:5]}\n",
    "\n",
    "def get_toxicity_scores(text: str) -> dict:\n",
    "    return {r[\"label\"]: r[\"score\"] for r in toxicity_classifier(text)[0]}\n",
    "\n",
    "def get_cyber_scores(text: str) -> dict:\n",
    "    return {r[\"label\"]: r[\"score\"] for r in cyberbullying_classifier(text)[0]}\n",
    "\n",
    "# 5.  Pretty console output\n",
    "def display_analysis(message: str) -> None:\n",
    "    emotions = get_top5_emotions(message)\n",
    "    toxicity = get_toxicity_scores(message)\n",
    "    cyber    = get_cyber_scores(message)\n",
    "\n",
    "    primary_emotion = max(emotions, key=emotions.get)\n",
    "    toxic_level     = toxicity.get(\"toxic\", 0.0)\n",
    "\n",
    "    if   toxic_level > 0.85: friendly = \"🔥 Highly Toxic\"\n",
    "    elif toxic_level > 0.50: friendly = \"⚠️ Possibly Offensive\"\n",
    "    elif toxic_level > 0.20: friendly = \"🟡 Mildly Risky\"\n",
    "    else:                    friendly = \"✅ Low or Safe\"\n",
    "\n",
    "    tags = []\n",
    "    if toxicity.get(\"insult\",        0) > 0.6: tags.append(\"🔴 Insult\")\n",
    "    if toxicity.get(\"obscene\",       0) > 0.6: tags.append(\"🤬 Obscene\")\n",
    "    if toxicity.get(\"severe_toxic\",  0) > 0.4: tags.append(\"🚨 Severe Toxicity\")\n",
    "    if toxicity.get(\"identity_hate\", 0) > 0.4: tags.append(\"🛑 Identity Hate\")\n",
    "    if toxicity.get(\"threat\",        0) > 0.3: tags.append(\"⚠️ Threat\")\n",
    "    tag_str = \", \".join(tags) if tags else \"No critical flags\"\n",
    "\n",
    "    print(\"\\n\" + \"─\" * 80)\n",
    "    print(textwrap.fill(message, 80))\n",
    "\n",
    "    print(\"\\n💡 Emotion Analysis (Top 5)\")\n",
    "    for lbl, sc in emotions.items():\n",
    "        print(f\"  {lbl:<18}{sc:6.3f}\")\n",
    "    print(f\"🧠 Primary Emotion: {primary_emotion}\")\n",
    "\n",
    "    print(\"\\n⚠️  General Toxicity\")\n",
    "    print(f\"  Level: {friendly}\")\n",
    "    print(f\"  Tags : {tag_str}\")\n",
    "\n",
    "    print(\"\\n🚨  Cyber-bullying Scores\")\n",
    "    for lbl, sc in cyber.items():\n",
    "        print(f\"  {lbl:<18}{sc:6.3f}\")\n",
    "\n",
    "    print(\"─\" * 80 + \"\\n\")\n",
    "\n",
    "# 6.  Simple CLI loop\n",
    "def main():\n",
    "    print(\"Type a message (or 'quit'):\\n\")\n",
    "    while True:\n",
    "        try:\n",
    "            msg = input(\"Message: \").strip()\n",
    "        except (EOFError, KeyboardInterrupt):\n",
    "            msg = \"quit\"\n",
    "        if msg.lower() == \"quit\":\n",
    "            print(\"Exiting. Thank you!\")\n",
    "            break\n",
    "        display_analysis(msg)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\textcolor{blue}{\\textbf{\\small \\text{Iteration 3 – Full-Stack Message Analyzer with Trigger Detection}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script integrates emotion detection, general toxicity analysis, and custom cyberbullying classification using pretrained and fine-tuned models. It adds trigger phrase detection to identify emotionally charged or harmful n-grams even when the full message seems subtle. The result is a powerful, interactive tool to analyze, explain, and interpret online communication risks in real time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Full-stack message analyser with trigger phrase detection\n",
    "────────────────────────────────────────────────────────\n",
    "• Emotions                 (SamLowe/roberta-base-go_emotions)\n",
    "• General toxicity         (unitary/toxic-bert)\n",
    "• Cyber-bullying detector  (jkos0012/bert-cyberbullying – .pth weights)\n",
    "\"\"\"\n",
    "\n",
    "import textwrap, torch\n",
    "from huggingface_hub import hf_hub_download\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoTokenizer,\n",
    "    BertConfig,\n",
    "    BertForSequenceClassification,\n",
    "    TextClassificationPipeline,\n",
    ")\n",
    "\n",
    "# ───────────────────────────────\n",
    "# Load emotion and toxicity models\n",
    "emotion_classifier = pipeline(\"text-classification\", model=\"SamLowe/roberta-base-go_emotions\", top_k=None)\n",
    "toxicity_classifier = pipeline(\"text-classification\", model=\"unitary/toxic-bert\", top_k=None)\n",
    "\n",
    "# ───────────────────────────────\n",
    "# Load cyberbullying model from .pth\n",
    "REPO_ID = \"jkos0012/bert-cyberbullying\"\n",
    "PT_FILE = \"bert_cyberbullying.pth\"\n",
    "BASE_MODEL = \"bert-base-uncased\"\n",
    "ckpt_path = hf_hub_download(REPO_ID, filename=PT_FILE)\n",
    "state_dict = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "\n",
    "num_labels = state_dict[\"classifier.weight\"].shape[0]\n",
    "LABELS = [\"religion\", \"age\"]\n",
    "id2label = dict(enumerate(LABELS))\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "config = BertConfig.from_pretrained(BASE_MODEL, num_labels=num_labels, id2label=id2label, label2id=label2id)\n",
    "cyber_model = BertForSequenceClassification(config)\n",
    "cyber_model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "cyberbullying_classifier = TextClassificationPipeline(\n",
    "    model=cyber_model,\n",
    "    tokenizer=AutoTokenizer.from_pretrained(BASE_MODEL),\n",
    "    function_to_apply=\"sigmoid\",\n",
    "    top_k=None,\n",
    ")\n",
    "\n",
    "# ───────────────────────────────\n",
    "# Helper functions\n",
    "\n",
    "def get_top5_emotions(text: str) -> dict:\n",
    "    res = emotion_classifier(text)[0]\n",
    "    res.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "    return {r[\"label\"]: r[\"score\"] for r in res[:5]}\n",
    "\n",
    "def get_toxicity_scores(text: str) -> dict:\n",
    "    return {r[\"label\"]: r[\"score\"] for r in toxicity_classifier(text)[0]}\n",
    "\n",
    "def get_cyber_scores(text: str) -> dict:\n",
    "    return {r[\"label\"]: r[\"score\"] for r in cyberbullying_classifier(text)[0]}\n",
    "\n",
    "def detect_trigger_phrases(text: str, threshold: float = 0.5, ngram_size: int = 2) -> dict:\n",
    "    words = [w for w in text.split() if w.isalpha()]\n",
    "    phrases = [' '.join(words[i:i + ngram_size]) for i in range(len(words) - ngram_size + 1)]\n",
    "\n",
    "    toxic_triggers = []\n",
    "    emotion_triggers = []\n",
    "\n",
    "    for phrase in phrases:\n",
    "        toxic_result = toxicity_classifier(phrase)[0]\n",
    "        toxic_score = next((r[\"score\"] for r in toxic_result if r[\"label\"] == \"toxic\"), 0)\n",
    "        if toxic_score > threshold:\n",
    "            toxic_triggers.append((phrase, toxic_score))\n",
    "\n",
    "        emotion_result = emotion_classifier(phrase)[0]\n",
    "        top_emotion = max(emotion_result, key=lambda x: x[\"score\"])\n",
    "        if top_emotion[\"score\"] > threshold:\n",
    "            emotion_triggers.append((phrase, top_emotion[\"label\"], top_emotion[\"score\"]))\n",
    "\n",
    "    return {\n",
    "        \"toxic_triggers\": toxic_triggers,\n",
    "        \"emotion_triggers\": emotion_triggers\n",
    "    }\n",
    "\n",
    "# ───────────────────────────────\n",
    "def display_analysis(message: str) -> None:\n",
    "    emotions = get_top5_emotions(message)\n",
    "    toxicity = get_toxicity_scores(message)\n",
    "    cyber = get_cyber_scores(message)\n",
    "    primary_emotion = max(emotions, key=emotions.get)\n",
    "    toxic_level = toxicity.get(\"toxic\", 0.0)\n",
    "\n",
    "    if toxic_level > 0.85: friendly = \"🔥 Highly Toxic\"\n",
    "    elif toxic_level > 0.50: friendly = \"⚠️ Possibly Offensive\"\n",
    "    elif toxic_level > 0.20: friendly = \"🟡 Mildly Risky\"\n",
    "    else: friendly = \"✅ Low or Safe\"\n",
    "\n",
    "    tags = []\n",
    "    if toxicity.get(\"insult\", 0) > 0.6: tags.append(\"🔴 Insult\")\n",
    "    if toxicity.get(\"obscene\", 0) > 0.6: tags.append(\"🤬 Obscene\")\n",
    "    if toxicity.get(\"severe_toxic\", 0) > 0.4: tags.append(\"🚨 Severe Toxicity\")\n",
    "    if toxicity.get(\"identity_hate\", 0) > 0.4: tags.append(\"🛑 Identity Hate\")\n",
    "    if toxicity.get(\"threat\", 0) > 0.3: tags.append(\"⚠️ Threat\")\n",
    "    tag_str = \", \".join(tags) if tags else \"No critical flags\"\n",
    "\n",
    "    print(\"\\n\" + \"─\" * 80)\n",
    "    print(textwrap.fill(message, 80))\n",
    "\n",
    "    print(\"\\n💡 Emotion Analysis (Top 5)\")\n",
    "    for lbl, sc in emotions.items():\n",
    "        print(f\"  {lbl:<18}{sc:6.3f}\")\n",
    "    print(f\"🧠 Primary Emotion: {primary_emotion}\")\n",
    "\n",
    "    print(\"\\n⚠️  General Toxicity\")\n",
    "    print(f\"  Level: {friendly}\")\n",
    "    print(f\"  Tags : {tag_str}\")\n",
    "\n",
    "    print(\"\\n🚨  Cyberbullying Scores\")\n",
    "    for lbl, sc in cyber.items():\n",
    "        print(f\"  {lbl:<18}{sc:6.3f}\")\n",
    "\n",
    "    # ✅ Trigger Phrase Detection (Only if text is flagged)\n",
    "    if toxic_level > 0.5 or any(score > 0.5 for score in cyber.values()):\n",
    "        triggers = detect_trigger_phrases(message)\n",
    "    else:\n",
    "        triggers = {\"toxic_triggers\": [], \"emotion_triggers\": []}\n",
    "\n",
    "    # ✅ Filter: show only strong toxic triggers (> 0.75)\n",
    "    toxic_trigs = [(p, s) for p, s in triggers[\"toxic_triggers\"] if s > 0.75]\n",
    "    emotion_trigs = triggers[\"emotion_triggers\"]\n",
    "\n",
    "    print(\"\\n🔎 Trigger Phrases:\")\n",
    "    if toxic_trigs:\n",
    "        print(\"  ⚠️ Toxicity Triggers:\")\n",
    "        for phrase, score in toxic_trigs:\n",
    "            print(f\"    - \\\"{phrase}\\\" (score: {score:.2f})\")\n",
    "    else:\n",
    "        print(\"  ✅ No strong toxicity trigger phrases detected.\")\n",
    "\n",
    "    # ✅ Show explanation if message is toxic but has no strong triggers\n",
    "    if toxic_level > 0.5 and len(toxic_trigs) == 0:\n",
    "        print(\"\\n🧠 Why this still matters:\")\n",
    "        print(\"  Although the individual words or short phrases in this message seem neutral,\")\n",
    "        print(\"  the overall sentence may still feel hurtful or bullying in certain social situations.\")\n",
    "        print(\"  Some expressions are used to dismiss, exclude, or insult — even without 'bad words'.\")\n",
    "        print(\"  Always think about how your words might make someone else feel.\")\n",
    "\n",
    "    if emotion_trigs:\n",
    "        print(\"  💬 Emotional Triggers:\")\n",
    "        for phrase, label, score in emotion_trigs:\n",
    "            print(f\"    - \\\"{phrase}\\\" → {label} (score: {score:.2f})\")\n",
    "    else:\n",
    "        print(\"  ✅ No strong emotion trigger phrases detected.\")\n",
    "\n",
    "    print(\"─\" * 80 + \"\\n\")\n",
    "    \n",
    "# ───────────────────────────────\n",
    "# Main CLI loop\n",
    "\n",
    "def main():\n",
    "    print(\"Type a message (or 'quit'):\\n\")\n",
    "    while True:\n",
    "        try:\n",
    "            msg = input(\"Message: \").strip()\n",
    "        except (EOFError, KeyboardInterrupt):\n",
    "            msg = \"quit\"\n",
    "        if msg.lower() == \"quit\":\n",
    "            print(\"Exiting. Thank you!\")\n",
    "            break\n",
    "        display_analysis(msg)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
